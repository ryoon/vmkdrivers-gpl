/* ****************************************************************
 * Portions Copyright 1998, 2009-2013, 2015 VMware, Inc.
 *
 * This program is free software; you can redistribute it and/or
 * modify it under the terms of the GNU General Public License
 * as published by the Free Software Foundation; either version 2
 * of the License, or (at your option) any later version.
 * ****************************************************************/

/******************************************************************
 *
 * linux_stubs.c
 *
 * From linux-2.4.31/kernel/panic.c:
 *
 * Copyright (C) 1991, 1992  Linus Torvalds
 *
 * From linux-2.6.18-8/kernel/resource.c:
 *
 * Copyright (C) 1999   Linus Torvalds
 * Copyright (C) 1999   Martin Mares <mj@ucw.cz>
 *
 * From linux-2.6.18-8/lib/cmdline.c:
 *
 * Code and copyrights come from init/main.c and arch/i386/kernel/setup.c
 *
 * From linux-2.6.18-8/init/main.c:
 *
 * Copyright (C) 1991, 1992  Linus Torvalds
 *
 * From linux-2.6.18-8/arch/i386/kernel/setup.c:
 *
 * Copyright (C) 1995  Linus Torvalds
 *
 * From linux-2.6.18-8/lib/iomap.c:
 *
 * (C) Copyright 2004 Linus Torvalds
 *
 * From linux-2.6.18-8/lib/vsprintf.c:
 *
 * Copyright (C) 1991, 1992  Linus Torvalds
 *
 * From linux-2.6.18.8/kernel/irq/manage.c:
 *
 * Copyright (C) 1992, 1998-2006 Linus Torvalds, Ingo Molnar
 * Copyright (C) 2005-2006 Thomas Gleixner
 *
 * From linux-2.6.28/lib/iomap.c
 *
 * (C) Copyright 2004 Linus Torvalds
 *
 * From linux-2.6.31.12/kernel/time.c:
 *
 * Copyright (C) 1991, 1992  Linus Torvalds
 *
 * From linux-2.6.18/arch/x86_64/mm/ioremap.c
 *
 * (C) Copyright 1995 1996 Linus Torvalds
 ******************************************************************/

#include <linux/pci.h>
#include <linux/interrupt.h>
#include <linux/spinlock.h>
#include <linux/ctype.h>
#include <linux/poll.h>
#include <linux/delay.h>
#include <linux/workqueue.h>
#include <linux/utsname.h>
#include <linux/firmware.h>
#include <linux/net.h>
#include <linux/random.h>
#include <linux/usb.h>
#include <linux/dmi.h>
#include <linux/miscdevice.h>
#include <linux/platform_device.h>
#include <asm/dmi.h>
#include <vmkplexer_mempool.h>
#include <scsi/scsi_cmnd.h>

#include "vmkapi.h"
#include "vmklinux_dist.h"
#include "linux_stubs.h"
#include "linux_pci.h"
#include "linux_irq.h"
#include "linux_task.h"
#include "linux_time.h"
#include "linux_kthread.h"
#include "linux_net.h"
#include "linux_efi.h"
#include "vmklinux_log.h"
#include "pm.h"

/* NOTE: __namespace.h is generated by the build from the driver's .sc file. */
#include "__namespace.h"

MODULE_VERSION2(VMKLNX_STRINGIFY(PRODUCT_VERSION), VMKLNX_MY_NAMESPACE_VERSION);
VMK_LICENSE_INFO(VMK_MODULE_LICENSE_GPLV2);

vmk_Bool is_vmvisor;

static vmk_Semaphore module_lock;
static vmk_ListLinks module_head;

/*
 * XXX: PR343324, PR322927
 * Gross hack: 
 * Mark buffers allocated with compat_alloc_user_space as VMKernel buffers
 * using the highest bit. This bit is still needed although the COS is gone,
 * the requirement for this VMKBUF_MASK hack remains until Util_CopyToUser()
 * knows how to recognize the heap memory that we're using here.
 */
#define VMKBUF_MASK		((1UL)<<63)

static void * 
__MarkPtrAsVMKBuffer(void *ptr)
{
   VMK_ASSERT(((vmk_VA)ptr & VMKBUF_MASK) == 0);
   return (void *)((vmk_VA)ptr | VMKBUF_MASK);
}

static vmk_Bool
__IsPtrVMKBuffer(const void *ptr)
{
   return (((vmk_VA)ptr & VMKBUF_MASK) == VMKBUF_MASK);
}

static void *
__SanitizeVMKBufferPtr(const void *ptr)
{
   return (void *)((vmk_VA)ptr & ~VMKBUF_MASK);
}

/*
 * The largest page frame number on the system
 */
vmk_MPN max_pfn;

/*
 * The number of PCPU is required by a few network drivers to optimize
 * multiqueue allocation.
 */
unsigned int smp_num_cpus;
EXPORT_SYMBOL(smp_num_cpus);

vmk_LogComponent  vmklinuxLog;

vmk_Semaphore     pci_bus_sem;
EXPORT_SYMBOL(pci_bus_sem);

#ifndef HAVE_ARCH_PIO_SIZE
/*
 * We encode the physical PIO addresses (0-0xffff) into the
 * pointer by offsetting them with a constant (0x10000) and
 * assuming that all the low addresses are always PIO. That means
 * we can do some sanity checks on the low bits, and don't
 * need to just take things for granted.
 */
#define PIO_OFFSET      0x10000UL
#define PIO_MASK        0x0ffffUL
#define PIO_RESERVED    0x40000UL
#endif

#define VERIFY_PIO(port) BUG_ON((port & ~PIO_MASK) != PIO_OFFSET)

#define IO_COND(addr, is_pio, is_mmio) do {                \
   unsigned long port = (unsigned long __force)addr;       \
   if (port < PIO_RESERVED) {                              \
      VERIFY_PIO(port);                                    \
      port &= PIO_MASK;                                    \
      is_pio;                                              \
   } else {                                                \
      is_mmio;                                             \
   }                                                       \
} while (0)

/*
 * Used by the sprintf->snprintf wrappers.  Dare we go
 * lower than 1 MB?
 */
#define SPRINTF_MAX_BUFLEN (1*1024*1024)

struct resource ioport_resource; 
EXPORT_SYMBOL(ioport_resource);
struct resource iomem_resource; 
EXPORT_SYMBOL(iomem_resource);

unsigned securebits = 0;

vmk_SpinlockIRQ irqMappingLock;
static vmk_SpinlockIRQ regionLock;

vmk_ModuleID vmklinuxModID;

static vmk_BHID linuxBHNum;

/* minimum time in jiffies between messages */
int printk_ratelimit_jiffies __read_mostly = 5 * HZ;

/* number of messages we send before ratelimiting */
int printk_ratelimit_burst __read_mostly = 10;

int net_msg_cost __read_mostly = 5*HZ;
int net_msg_burst __read_mostly = 10;
int net_msg_warn __read_mostly = 1;

#define MAX_SOFTIRQ	32 
static struct softirq_action softirq_vec[MAX_SOFTIRQ]
   VMK_ATTRIBUTE_L1_ALIGNED;

typedef struct LinuxBHData {
   struct LinuxBHData *next;
   void (*routine)(void *);
   void *data;
   vmk_ModuleID modID;
   vmk_Bool staticAlloc;
} LinuxBHData;

// Set MIN size to 1MB as there are requests to vmalloc of upto 512K during
// drivers load time and so setting 512K or less may cause heap size to end
// up at more than 8MB as heap grows as percentage of MAX size and we want
// to avoid that situation.
#define VMKLNX_VMALLOC_HEAP_MIN   (1024*1024)
#define VMKLNX_VMALLOC_HEAP_MAX   (160*1024*1024)
#define VMKLNX_VMALLOC_HEAP       "vmklnxVmallocHeap"
vmk_HeapID vmklnxVmallocHeap = VMK_INVALID_HEAP_ID;
static vmk_MemPool mem_pool = VMK_MEMPOOL_INVALID;

// Define module load time paramaters for vmalloc's heap	
static int vmklnx_vmalloc_heap_min = VMKLNX_VMALLOC_HEAP_MIN;
module_param(vmklnx_vmalloc_heap_min, int, 0444);
MODULE_PARM_DESC(vmklnx_vmalloc_heap_min, "Initial heap size allocated for vmalloc.");

static int vmklnx_vmalloc_heap_max = VMKLNX_VMALLOC_HEAP_MAX;
module_param(vmklnx_vmalloc_heap_max, int, 0444);
MODULE_PARM_DESC(vmklnx_vmalloc_heap_max, "Maximum attainable heap size for vmalloc.");

static int vmklnx_low_port2numbering = 0;
module_param(vmklnx_low_port2numbering, int, 0444);
MODULE_PARM_DESC(vmklnx_low_port2numbering, "Lower port2 vmnic aliases.");

#define VMKLNX_LOW_HEAP_MIN   (1*1024*1024)
#define VMKLNX_LOW_HEAP_MAX   (20*1024*1024)
#define VMKLNX_LOW_HEAP       "vmklnxLowHeap"
vmk_HeapID vmklnxLowHeap = VMK_INVALID_HEAP_ID;

// "vmklnxEmergencyHeap" is dedicated to allocate memory for any emergency case.
// Currently it only used for swap IO scsi command.
// Please increase the size carefully if you also use it for other purpose.
// HeapSize =  sizeof(struct scsi_cmnd) * NumberOfOutStandingSwapIO;
#define NUM_OUTSTANDING_SWAP_IO   4096
#define VMKLNX_EMERGENCY_HEAP_MIN   (sizeof(struct scsi_cmnd) * NUM_OUTSTANDING_SWAP_IO)
#define VMKLNX_EMERGENCY_HEAP_MAX   VMKLNX_EMERGENCY_HEAP_MIN
#define VMKLNX_EMERGENCY_HEAP       "vmklnxEmergencyHeap"
vmk_HeapID vmklnxEmergencyHeap = VMK_INVALID_HEAP_ID;

// Define module load time paramaters for the vmklinux low heap 
static int vmklnx_low_heap_min = VMKLNX_LOW_HEAP_MIN;
module_param(vmklnx_low_heap_min, int, 0444);
MODULE_PARM_DESC(vmklnx_low_heap_min, "Initial heap size allocated for low.");

static int vmklnx_low_heap_max = VMKLNX_LOW_HEAP_MAX;
module_param(vmklnx_low_heap_max, int, 0444);
MODULE_PARM_DESC(vmklnx_low_heap_max, "Maximum attainable heap size for low.");

/*
 * PER_PCPU_VMKLINUX_DATA_CACHE_LINES is the number of 128 byte cache-lines
 *   needed to hold non-pad content of vmkLinuxPCPU_t.
 */
#define PER_PCPU_VMKLINUX_DATA_CACHE_LINES 1

typedef struct vmkLinuxPCPU_t {
   union {
      struct {
         LinuxBHData *linuxBHList VMK_ATTRIBUTE_L1_ALIGNED;
         vmk_atomic64 softirq_pending;
         LinuxBHData softirqLinuxBHData;
      };
      char pad[PER_PCPU_VMKLINUX_DATA_CACHE_LINES*L1_CACHE_BYTES] 
           VMK_ATTRIBUTE_L1_ALIGNED;
   } VMK_ATTRIBUTE_L1_ALIGNED;
} vmkLinuxPCPU_t VMK_ATTRIBUTE_L1_ALIGNED;

static vmkLinuxPCPU_t *vmkLinuxPCPU;

struct new_utsname system_utsname = {
	.sysname	= "vmklinux_9",
	.nodename       = "(none)",  
	.release	= "",
	.version	= "",
	.machine	= "",
	.domainname	= "",
};
EXPORT_SYMBOL(system_utsname);

inline struct new_utsname *init_utsname(void)
{
   return &system_utsname;
}
EXPORT_SYMBOL(init_utsname);

/* Config option handles for dump parameters */
static vmk_ConfigParamHandle dumpPollRetriesHandle;
static vmk_ConfigParamHandle dumpPollDelayHandle;

/* DMI related */
int dmi_alloc_index;
char dmi_alloc_data[DMI_MAX_DATA];
/*
 * The rtnl_mutex is the only portion of rtnetlink.c we retain.
 */
DEFINE_MUTEX(vmklnx_rtnl_mutex);
EXPORT_SYMBOL(vmklnx_rtnl_mutex);

/*
 * Support for compat_alloc_user_space.
 */
struct umem {
   long len;

   /*
    * Must come last.  The actual size of the data field is
    * dynamically determined at run time.
    */
   char data[1];
};

/*
 * Forward declarations.
 */
static VMK_ReturnStatus vmklnx_mod_init(void);
static void vmklnx_mod_uninit(void);

#define MIN_UMEM_SIZE   128

 /*
  *----------------------------------------------------------------------
  *
  * vmklnx_get_low_port2numbering --
  *
  *      Return the value of vmklinux_low_port2numbering
  *
  *----------------------------------------------------------------------
  */
unsigned int vmklnx_get_low_port2numbering(void)
{
   return vmklnx_low_port2numbering;
}

/*
 *----------------------------------------------------------------------
 *
 * vmklnx_get_dump_poll_retries --
 *
 *      Get the number of device retries to use when doing a coredump 
 *
 *----------------------------------------------------------------------
 */
unsigned int
vmklnx_get_dump_poll_retries(void)
{
   unsigned int value;

   VMK_ASSERT(vmk_PreemptionIsEnabled() == VMK_FALSE);
   vmk_ConfigParamGetUint(dumpPollRetriesHandle, &value);
   return value;
}
EXPORT_SYMBOL(vmklnx_get_dump_poll_retries);

/*
 *----------------------------------------------------------------------
 *
 * vmklnx_get_dump_poll_delay --
 *
 *      Get the number of microseconds to delay between poll attempts
 *      when doing a coredump 
 *
 *----------------------------------------------------------------------
 */
unsigned int
vmklnx_get_dump_poll_delay(void)
{
   unsigned int value;

   VMK_ASSERT(vmk_PreemptionIsEnabled() == VMK_FALSE);
   vmk_ConfigParamGetUint(dumpPollDelayHandle, &value);
   return value;
}
EXPORT_SYMBOL(vmklnx_get_dump_poll_delay);

/*
 *----------------------------------------------------------------------
 *
 * vmklnx_errno_to_vmk_return_status --
 *
 *      Convert a linux errno to a VMK_ReturnStatus
 *
 *----------------------------------------------------------------------
 */
VMK_ReturnStatus
vmklnx_errno_to_vmk_return_status(int error)
{
    return ((error) == 0 ? VMK_OK :
            (error) <  0 ? VMK_GENERIC_LINUX_ERROR - (error) :
                           VMK_GENERIC_LINUX_ERROR + (error));
}

/*
 *----------------------------------------------------------------------
 *
 * vmklnx_phys_to_kmap --
 *
 *      Stub routine that maps a machine address to a kernel virtual
 *      address. Need to use this where the machine address could have
 *      come from the host
 *      Note: length cannot be more than one page
 *
 * Results: 
 *	Virtual address pointing to the machine address
 *
 *----------------------------------------------------------------------
 */
void *
vmklnx_phys_to_kmap(vmk_uint64 maddr, vmk_uint32 length)
{
   VMK_ReturnStatus status;
   vmk_VA vaddr;
   vmk_MpnRange range;
   vmk_MapRequest mRequest;

   VMK_ASSERT(length > 0 && length <= PAGE_SIZE);

   range.startMPN = vmk_MA2MPN(maddr);
   range.numPages = vmk_MA2MPN(maddr + length - 1) - range.startMPN + 1;
   mRequest.mapType = VMK_MAPTYPE_DEFAULT;
   mRequest.mapAttrs = VMK_MAPATTRS_READWRITE;
   mRequest.mpnRanges = &range;
   mRequest.numElements = 1;
   status = vmk_Map(vmklinuxModID, &mRequest, &vaddr);
   if (status != VMK_OK) {
      return NULL;
   }
   
   return (void *)vaddr;
}

/*
 *----------------------------------------------------------------------
 *
 * vmklnx_phys_to_kmap_free --
 *
 *      Stub routine that frees a virtual to machine mapping 
 *
 * Results: 
 *	none
 *
 * Side effects:
 *	
 *
 *----------------------------------------------------------------------
 */
void
vmklnx_phys_to_kmap_free(void *vaddr)
{
   vmk_Unmap((vmk_VA)vaddr);
}

/*
 *----------------------------------------------------------------------
 *
 * vmklnx_p2v_memcpy --
 * 
 *    This routine will copy data from physical address 'src' to virtual
 *    address 'dst', in VMK_PAGE_SIZE chunks.  This is necessary because
 *    vmklnx_phys_to_kmap is limited to mapping PAGE_SIZE bytes of memory for 
 *    unmapped physical addresses.  
 *
 * Results:
 *    'dst' contains data copied from 'src' of length 'length'.
 *
 * Side effects:
 *    Briefly allocates a mapping in kseg.  
 *
 *----------------------------------------------------------------------
 */
inline void 
vmklnx_p2v_memcpy(void *dst, vmk_uint64 src, vmk_uint32 length)
{
   void *src_virt;
   uint32_t left = length;
   
   /*
    * XXX: This doesn't check VA validity. Rather this might lead to stray PSODs
    * because vmk_VA2MA is not safe to be called with an invalid VA.
    */
#if 0
   // Check validity of dst VA. 
   if (vmk_VA2MA((vmk_VA)dst & PAGE_MASK, VMK_PAGE_SIZE, &tmp) != VMK_OK) {
      VMKLNX_DEBUG(0, "Invalid dst addr %p", dst); 
   }
#endif

   while(left) {
      uint32_t xfer_sz = min(left, (uint32_t)VMK_PAGE_SIZE);
      uint32_t offset = length - left;

      src_virt = vmklnx_phys_to_kmap(src + offset, xfer_sz);
      memcpy(dst + offset, src_virt, xfer_sz);
      vmklnx_phys_to_kmap_free(src_virt);
      left -= xfer_sz;
   }
}

/*
 *----------------------------------------------------------------------
 *
 * vmklnx_v2p_memcpy --
 * 
 *    This routine will copy data from virtual address 'src' to physical
 *    address 'dst', in VMK_PAGE_SIZE chunks.  This is necessary because
 *    vmklnx_phys_to_kmap is limited to mapping PAGE_SIZE bytes of memory for 
 *    unmapped physical addresses.  
 *
 * Results:
 *    'dst' contains data copied from 'src' of length 'length'.
 *
 * Side effects:
 *    Briefly allocates a mapping in kseg.  
 *
 *----------------------------------------------------------------------
 */
inline void 
vmklnx_v2p_memcpy(vmk_uint64 dst, void *src, vmk_uint32 length)
{
   void *dst_virt;

   uint32_t left = length;

   /*
    * XXX: This doesn't check VA validity. Rather this might lead to stray PSODs
    * because vmk_VA2MA is not safe to be called with an invalid VA.
    */
#if 0
   // Check validity of src VA. 
   if (vmk_VA2MA((vmk_VA)src & PAGE_MASK, VMK_PAGE_SIZE , &tmp) != VMK_OK) {
      VMKLNX_DEBUG(0, "Invalid src addr %p", src); 
   }
#endif

   while(left) {
      uint32_t xfer_sz = min(left, (uint32_t)VMK_PAGE_SIZE);
      uint32_t offset = length - left;

      dst_virt = vmklnx_phys_to_kmap(dst + offset, xfer_sz);
      memcpy(dst_virt, src + offset, xfer_sz);
      vmklnx_phys_to_kmap_free(dst_virt);
      left -= xfer_sz;
   }
}

/*
 * Remap an arbitrary physical address space into the kernel virtual
 * address space. Needed when the kernel wants to access high addresses
 * directly.
 */
void __iomem *
__ioremap(unsigned long phys_addr, unsigned long size, unsigned long flags)
{
   VMK_ReturnStatus status;
   void *addr;
   unsigned long last_addr;
   vmk_MpnRange range;
   vmk_MapRequest mRequest;

   VMK_ASSERT(vmk_PreemptionIsEnabled() == VMK_FALSE);
   VMKLNX_DEBUG(4, "phys_addr 0x%lx size 0x%lx flags 0x%lx", phys_addr, size, flags);

   /*
    * No zero size or terminating address that wraps around
    */
   last_addr = phys_addr + size - 1;
   if (size == 0 || phys_addr == 0 || last_addr < phys_addr) {
      return NULL;
   }

   range.startMPN = vmk_MA2MPN(phys_addr);
   range.numPages = vmk_MA2MPN(last_addr) - range.startMPN + 1;
   mRequest.mapType = VMK_MAPTYPE_DEFAULT;
   mRequest.mapAttrs = VMK_MAPATTRS_READWRITE;
   mRequest.mpnRanges = &range;
   mRequest.numElements = 1;
   if (flags & _PAGE_PCD) {
      mRequest.mapAttrs |= VMK_MAPATTRS_UNCACHED;
   }
   status = vmk_Map(vmklinuxModID, &mRequest, (vmk_VA *)&addr);
   if (status == VMK_OK) {
      /*
       * Remember, linux's PAGE_MASK is (~(PAGE_SIZE - 1)) instead
       * of just PAGE_SIZE like vmkernel.
       */
      addr += phys_addr & ~PAGE_MASK;
   }
   else {
      /*
       * Ensure that drivers can check against NULL for failure 
       */
      addr = NULL;
   }

   VMK_ASSERT(addr != NULL);

   VMKLNX_DEBUG(4, "returning %p", addr);

   return addr;
}
EXPORT_SYMBOL(__ioremap);

/**                                          
 *  ioremap_nocache - perform an uncacheable mapping of physical memory
 *  @offset: physical address to map   
 *  @size: number of bytes to map
 *                                           
 *  Map in a physically contiguous range into kernel virtual memory and
 *  get a pointer to the mapped region. The region is mapped uncacheable.      
 *                                           
 *  RETURN VALUE:
 *     None.
 *                                           
 */                                         
/* _VMKLNX_CODECHECK_: ioremap_nocache */
void __iomem * 
ioremap_nocache (unsigned long offset, unsigned long size)
{
   VMK_ASSERT(vmk_PreemptionIsEnabled() == VMK_FALSE);
   return __ioremap(offset, size, _PAGE_PCD);
}
EXPORT_SYMBOL(ioremap_nocache);


/**                                          
 *  iounmap - unmap a previously mapped physically contiguous region       
 *  @addr: virtual address of the mapping to unmap   
 *                                           
 *  Unmap a physically contiguous region mapped into the kernel by
 *  ioremap or ioremap_nocache.
 *                                           
 *  RETURN VALUE:
 *     None.
 *                                           
 */                                          
/* _VMKLNX_CODECHECK_: iounmap */
void
iounmap(volatile void __iomem *addr)
{
   VMK_ASSERT(vmk_PreemptionIsEnabled() == VMK_FALSE);
   VMKLNX_DEBUG(4, "Unmapping %p", addr);
   vmk_Unmap((vmk_VA)((unsigned long)addr & PAGE_MASK));
}
EXPORT_SYMBOL(iounmap);

/* Create a virtual mapping cookie for an IO port range */
void __iomem *
ioport_map(unsigned long port, unsigned int nr)
{
   if (port > PIO_MASK)
      return NULL;
   return (void __iomem *) (unsigned long) (port + PIO_OFFSET);
}

void 
ioport_unmap(void __iomem *addr)
{
   /* Nothing to do */
}

/* Create a virtual mapping cookie for a PCI BAR (memory or IO) */
/**                                          
 *  pci_iomap -  Perform a virtual mapping for a PCI BAR (memory or IO)
 *  @dev:  pointer to the PCI device data structure
 *  @bar:  the BAR (base address register) whose access is sought
 *  @maxlen: length of the memory requested
 *                                           
 *  Map in a contiguous physical memory region corresponding to the address in 
 *  BAR @bar of PCI device @dev with size @maxlen to a virtual address.
 *                                           
 *  Return Value:
 *  address if successful
 *  NULL if failed
 *                                           
 */                                          
/* _VMKLNX_CODECHECK_: pci_iomap */
void __iomem *
pci_iomap(struct pci_dev *dev, int bar, unsigned long maxlen)
{
   unsigned long start = pci_resource_start(dev, bar);
   unsigned long len = pci_resource_len(dev, bar);
   unsigned long flags = pci_resource_flags(dev, bar);

   VMK_ASSERT(vmk_PreemptionIsEnabled() == VMK_FALSE);
   if (!len || !start)
      return NULL;
   if (maxlen && len > maxlen)
      len = maxlen;
   if (flags & IORESOURCE_IO)
      return ioport_map(start, len);
   if (flags & IORESOURCE_MEM) {
      if (flags & IORESOURCE_CACHEABLE)
         return ioremap(start, len);
      return ioremap_nocache(start, len);
   }
   /* What? */
   return NULL;
}
EXPORT_SYMBOL(pci_iomap);

/**                                          
 *  pci_iounmap - unmap a previously mapped physically contiguous PCI region
 *  @dev: pointer to the PCI device data structure
 *  @addr: virtual address of the mapping to unmap
 *                                           
 *  Unmap a physically contiguous region mapped into the kernel by pci_iomap
 *                                           
 *  Return Value:
 *  Does not return any value                                           
 */                                          
/* _VMKLNX_CODECHECK_: pci_iounmap */
void 
pci_iounmap(struct pci_dev *dev, void __iomem * addr)
{
   VMK_ASSERT(vmk_PreemptionIsEnabled() == VMK_FALSE);
   IO_COND(addr, /* nothing */, iounmap(addr));
}
EXPORT_SYMBOL(pci_iounmap);

/* from lib/iomap.c */
/**                                          
 *  ioread8 - Read 8-bits from an IO memory address     
 *  @addr: IO address to read     
 *                                           
 *  Reads 8-bits from the specified IO memory address and returns the
 *  value as an integer.                      
 *                                           
 *  RETURN VALUE:
 *    8-bits from the specified IO memory address
 *                                           
 */                                          
/* _VMKLNX_CODECHECK_: ioread8 */
unsigned int fastcall
ioread8(void __iomem *addr)
{
   VMK_ASSERT(vmk_PreemptionIsEnabled() == VMK_FALSE);
   IO_COND(addr, return inb(port), return readb(addr));
}
EXPORT_SYMBOL(ioread8);

/**                                          
 *  ioread16 - Read 16-bits from an IO memory address     
 *  @addr: IO address to read     
 *                                           
 *  Reads 16-bits from the specified IO memory address and returns the
 *  value as an integer.                      
 *                                           
 *  RETURN VALUE:
 *    16-bits from the specified IO memory address
 *                                           
 */                                          
/* _VMKLNX_CODECHECK_: ioread16 */
unsigned int fastcall
ioread16(void __iomem *addr)
{
   VMK_ASSERT(vmk_PreemptionIsEnabled() == VMK_FALSE);
   IO_COND(addr, return inw(port), return readw(addr));
}
EXPORT_SYMBOL(ioread16);

/**                                          
 *  ioread32 - Read 32-bits from an IO memory address     
 *  @addr: IO address to read     
 *                                           
 *  Reads 32-bits from the specified IO memory address and returns the
 *  value as an integer.                      
 *                                           
 *  RETURN VALUE:
 *    32-bits from the specified IO memory address
 *                                           
 */                                          
/* _VMKLNX_CODECHECK_: ioread32 */
unsigned int fastcall 
ioread32(void __iomem *addr)
{
   VMK_ASSERT(vmk_PreemptionIsEnabled() == VMK_FALSE);
   IO_COND(addr, return inl(port), return readl(addr));
}
EXPORT_SYMBOL(ioread32);

/**                                          
 *  iowrite8 - Write 8-bits to an IO memory address
 *  @val: Value to to write.
 *  @addr: IO address to write to.
 *                                           
 *  Writes 8-bits to the specified IO memory address.                     
 *                                           
 *  RETURN VALUE:
 *    None.
 *                                           
 */                                         
/* _VMKLNX_CODECHECK_: iowrite8 */
void fastcall
iowrite8(u8 val, void __iomem *addr)
{
   VMK_ASSERT(vmk_PreemptionIsEnabled() == VMK_FALSE);
   IO_COND(addr, outb(val,port), writeb(val, addr));
}
EXPORT_SYMBOL(iowrite8);

/**                                          
 *  iowrite16 - Write 16-bits to an IO memory address
 *  @val: Value to to write.
 *  @addr: IO address to write to.
 *                                           
 *  Writes 16-bits to the specified IO memory address.                     
 *                                           
 *  RETURN VALUE:
 *    None.
 *                                           
 */                                         
/* _VMKLNX_CODECHECK_: iowrite16 */
void fastcall
iowrite16(u16 val, void __iomem *addr)
{
   VMK_ASSERT(vmk_PreemptionIsEnabled() == VMK_FALSE);
   IO_COND(addr, outw(val,port), writew(val, addr));
}
EXPORT_SYMBOL(iowrite16);

/**                                          
 *  iowrite32 - Write 32-bits to an IO memory address
 *  @val: Value to to write.
 *  @addr: IO address to write to.
 *                                           
 *  Writes 32-bits to the specified IO memory address.                     
 *                                           
 *  RETURN VALUE:
 *    None.
 *                                           
 */                                         
/* _VMKLNX_CODECHECK_: iowrite32 */
void fastcall 
iowrite32(u32 val, void __iomem *addr)
{
   VMK_ASSERT(vmk_PreemptionIsEnabled() == VMK_FALSE);
   IO_COND(addr, outl(val,port), writel(val, addr));
}
EXPORT_SYMBOL(iowrite32);

static inline void
mmio_insw(void __iomem *addr, u16 *dst, int count)
{
   while (--count >= 0) {
      u16 data = __raw_readw(addr);
      *dst = data;
      dst++;
   }
}

static inline void
mmio_outsw(void __iomem *addr, const u16 *src, int count)
{
   while (--count >= 0) {
      __raw_writew(*src, addr);
      src++;
   }
}

/**
 *  ioread16_rep - Read number of 16-bits from an IO memory address
 *  @addr: IO address to read from.
 *  @dst:  Pointer to the destination buffer.
 *  @count: Number of 16-bits to read.
 *
 *  Read number of 16-bits from an IO memory address.
 *
 *  RETURN VALUE:
 *    None.
 *
 */
/* _VMKLNX_CODECHECK_: ioread16_rep */
void fastcall
ioread16_rep(void __iomem *addr, void *dst, unsigned long count)
{
   VMK_ASSERT(vmk_PreemptionIsEnabled() == VMK_FALSE);
   IO_COND(addr, insw(port,dst,count), mmio_insw(addr, dst, count));
}
EXPORT_SYMBOL(ioread16_rep);

/**
 *  iowrite16_16 - Write number of 16-bits to an IO memory address
 *  @addr: IO address to write to.
 *  @src:  Pointer to the source buffer.
 *  @count: Number of 16-bits to write.
 *
 *  Write number of 16-bits to an IO memory address.
 *
 *  RETURN VALUE:
 *    None.
 *
 */
/* _VMKLNX_CODECHECK_: iowrite16_rep */
void fastcall
iowrite16_rep(void __iomem *addr, const void *src, unsigned long count)
{
   VMK_ASSERT(vmk_PreemptionIsEnabled() == VMK_FALSE);
   IO_COND(addr, outsw(port, src, count), mmio_outsw(addr, src, count));
}
EXPORT_SYMBOL(iowrite16_rep);

/**                                          
 *  __free_pages - releases pages
 *  @page: starting page handle
 *  @order: logarithm of (number of contiguous pages requested) to base 2
 *                                           
 *  Releases the pages allocated using alloc_pages and alloc_page.
 *  
 *  RETURN VALUE:
 *  NONE
 */                                          
/* _VMKLNX_CODECHECK_: __free_pages */
fastcall void 
__free_pages(struct page *page, unsigned int order)
{
   vmk_uint32 numPages = (1 << order);
   vmk_MPN pfn;

   VMK_ASSERT(vmk_PreemptionIsEnabled() == VMK_FALSE);
   pfn = page_to_pfn(page);

   vmk_PktSlabFreePages(pfn, numPages);
}
EXPORT_SYMBOL(__free_pages);

/**                                          
 *  free_pages - releases pages
 *  @vaddr: virtual address
 *  @order: logarithm of (number of contiguous pages requested) to base 2
 *             
 *  Releases the pages allocated by __get_free_pages.
 *                            
 *  RETURN VALUE:
 *  NONE
 */                                          
/* _VMKLNX_CODECHECK_: free_pages */
fastcall void
free_pages(unsigned long vaddr, unsigned int order)
{
   VMK_ASSERT(vmk_PreemptionIsEnabled() == VMK_FALSE);
   if (likely(vaddr)) {
      __free_pages(virt_to_page(vaddr), order);
   }
}
EXPORT_SYMBOL(free_pages);

/**
  *  alloc_pages - allocate pages and return the page descriptor
  *  @gfp_mask: a bit mask of flags indicating the required memory properties
  *  @order: size in the power of 2
  *
  *  Allocates 2^@order memory in contiguous pages and returns the page descriptor for it.
  *
  * ESX Deviation notes:
  * gfp_mask is ignored.
  *
  * The resulting pointer to the page descriptor should not be referenced nor
  * used in any form of pointer arithmetic to obtain the page descriptor to
  * any adjacent page. The pointer should be treated as an opaque handle and
  * should only be used as argument to other functions.
  *
  * SEE ALSO:
  * __free_pages
  * virt_to_page
  *
  * RETURN VALUE
  * a pointer to struct page if successful, NULL otherwise.
  */
/* _VMKLNX_CODECHECK_: alloc_pages */
fastcall struct page *
alloc_pages(unsigned int gfp_mask, unsigned int order)
{
   VMK_ReturnStatus status;
   vmk_uint32 numPages = (1 << order);
   vmk_MPN pfn;

   VMK_ASSERT(vmk_PreemptionIsEnabled() == VMK_FALSE);
   /*
    * We don't support request for allocation in the
    * first 16M memory region.
    */
   VMK_ASSERT((gfp_mask & GFP_DMA) == 0);

   if (gfp_mask & (GFP_DMA32)) {
      status = vmk_PktSlabAllocPages(numPages, VMK_PHYS_ADDR_BELOW_4GB, &pfn);
   } else {
      status = vmk_PktSlabAllocPages(numPages, VMK_PHYS_ADDR_ANY, &pfn);
   }

   if (unlikely(status != VMK_OK)) {
         static uint32_t allocThrottleCounter = 0;
         VMKLNX_THROTTLED_INFO(allocThrottleCounter, 
                  "gfp_mask=0x%x, order=0x%x, vmk_PktSlabAllocPage returned '%s'",
                  gfp_mask,
                  order,
                  vmk_StatusToString(status));
      return NULL;
   }

   return pfn_to_page((unsigned long)pfn);
}
EXPORT_SYMBOL(alloc_pages);

/**
 *  __get_free_pages - allocates pages
 *  @gfp_mask: allocation options
 *  @order: order of pages
 *
 *  Allocates pages and returns a virtual address to the beginning of the allocated region.
 *
 *  ESX Deviation Notes:
 *  None.
 *
 */
/* _VMKLNX_CODECHECK_: __get_free_pages */
fastcall unsigned long
__get_free_pages(gfp_t gfp_mask, unsigned int order)
{
   struct page *page;

   VMK_ASSERT(vmk_PreemptionIsEnabled() == VMK_FALSE);
   page = alloc_pages(gfp_mask, order);
   if (unlikely(!page)) {
      return 0;
   }
   return (unsigned long)page_address(page);
}
EXPORT_SYMBOL(__get_free_pages);

typedef struct Region {
   struct Region	*next;
   resource_size_t	base;
   resource_size_t	length;
   char			*name;
} Region;

static Region *regions;

static void
AddRegion(Region *prev, unsigned long from, unsigned long extent, const char *name)
{
   Region *r = (Region *)vmk_HeapAlloc(VMK_MODULE_HEAP_ID, sizeof(Region));
   if (r == NULL) {
      VMKLNX_WARN("Out of memory");
      return;
   }
   r->base = from;
   r->length = extent;
   r->name = vmk_HeapAlloc(VMK_MODULE_HEAP_ID, strlen(name) + 1);
   if (r->name == NULL) {
      VMKLNX_WARN("Out of memory");
      vmk_HeapFree(VMK_MODULE_HEAP_ID, r);
      return;
   }
   memcpy(r->name, name, strlen(name) + 1);

   if (prev == NULL) {
      r->next = regions;
      regions = r;
   } else {
      r->next = prev->next;
      prev->next = r;
   }
}

/**                                          
 *  __check_region - check if a resource region is busy or free
 *  @parent: parent resource descriptor
 *  @from: resource start address
 *  @extent: resource region size
 *
 *  Checks if a resource region is busy or free
 *                                           
 *  NOTES:
 *  This function is deprecated because its use may result in race condition.
 *  Even if it returns 0, a subsequent call to request_region may fail because
 *  another driver etc. just allocated the region. 
 *  Do NOT use it. It will be removed from the kernel. 
 *
 *  RETURN VALUE:
 *  Returns 0 if the region is free at the moment it is checked, non-zero otherwise.
 *                                           
 */                                          
/* _VMKLNX_CODECHECK_: __check_region */
int __check_region(struct resource *parent, resource_size_t from, resource_size_t extent)
{
   Region *r;
   uint64_t prevIRQL;
   int status = 0;

   VMK_ASSERT(vmk_PreemptionIsEnabled() == VMK_FALSE);
   VMKLNX_DEBUG(2, "0x%Lx for 0x%Lx", from, extent);

   prevIRQL = vmk_SPLockIRQ(&regionLock);

   for (r = regions; r != NULL; r = r->next) {
      if ((from >= r->base && from < r->base + r->length) ||
            (from < r->base && from + extent > r->base)) {
         VMKLNX_DEBUG(2, "Region conflict");
         status = 1;
         break;
      } else if (from < r->base) {
         break;
      }
   }

   vmk_SPUnlockIRQ(&regionLock, prevIRQL);

   return status;
}
EXPORT_SYMBOL(__check_region);

/**                                          
 *  __request_region - Reserve a region within a resource
 *  @parent: parent resource descriptor
 *  @from: resource start address
 *  @extent: resource region size
 *  @name: reserving caller's ID string
 * 
 *                                           
 *  ESX Deviation Notes:                     
 *  Always returns (struct resource *) 1
 *                                           
 */                                          
/* _VMKLNX_CODECHECK_: __request_region */
struct resource * __request_region(struct resource *parent, resource_size_t from, resource_size_t extent, const char *name)
{
   Region *r, *prev;
   uint64_t prevIRQL;

   VMK_ASSERT(vmk_PreemptionIsEnabled() == VMK_FALSE);
   VMKLNX_DEBUG(2, "0x%Lx for 0x%Lx named %s", from, extent, name);

   prevIRQL = vmk_SPLockIRQ(&regionLock);

   for (prev = NULL, r = regions;;  prev = r, r = r->next) {
      if (r == NULL) {
         AddRegion(prev, from, extent, name);
         vmk_SPUnlockIRQ(&regionLock, prevIRQL);
         break;
      } else if ((from >= r->base && from < r->base + r->length) ||
            (from < r->base && from + extent > r->base)) { 
         static int throttle = 0;

         vmk_SPUnlockIRQ(&regionLock, prevIRQL);
         VMKLNX_THROTTLED_WARN(throttle,
                               "Region conflict @ 0x%Lx => 0x%Lx",
                               from,
                               from + extent - 1);
         break;
      } else if (from < r->base) {
         AddRegion(prev, from, extent, name);
         vmk_SPUnlockIRQ(&regionLock, prevIRQL);
         break;
      }
   }

   // all three loop exits drop the regionLock
   vmk_SPAssertIsUnlockedIRQ(&regionLock);

   /* Drivers check for the return value to not be NULL, but they currently 
    * don't use it.  If they ever do, returning a 1 here, should allow 
    * us to track down the problem quickly.
    */
   return (struct resource *)1;
}
EXPORT_SYMBOL(__request_region);

/**                                          
 *  __release_region - release a previously reserved resource region     
 *  @parent: parent resource descriptor
 *  @from: resource start address
 *  @extent: resource region size
 *                                           
 *  The described resource region must match a currently busy region
 *                                           
 *  ESX Deviation Notes:
 *  No warning is printed when region is not found. 
 */                                          
/* _VMKLNX_CODECHECK_: __release_region */
void __release_region(struct resource *parent, resource_size_t from, resource_size_t extent)
{
   Region *r, *prev;
   uint64_t prevIRQL;

   VMK_ASSERT(vmk_PreemptionIsEnabled() == VMK_FALSE);
   VMKLNX_DEBUG(2, "0x%Lx for 0x%Lx", from, extent);

   prevIRQL = vmk_SPLockIRQ(&regionLock);

   for (prev = NULL, r = regions; r != NULL; prev = r, r = r->next) {
      if (from == r->base && extent == r->length) {
         if (prev == NULL) {
            regions = r->next;
         } else {
            prev->next = r->next;
         }
         vmk_HeapFree(VMK_MODULE_HEAP_ID, r->name);
         vmk_HeapFree(VMK_MODULE_HEAP_ID, r);
         break;
      }
   }

   if (r == NULL) {
      // Don't print message, since release_region() could have
      // already been called by the driver.
      //VMKLNX_DEBUG(0, "Couldn't find region 0x%x for %d", from, extent);
   }

   vmk_SPUnlockIRQ(&regionLock, prevIRQL);
}
EXPORT_SYMBOL(__release_region);

/*
 *----------------------------------------------------------------------------
 *
 *  Linux_PollBH --
 *
 *    Bottom half to handle poll notification requests.
 *
 *  Results:
 *    None.
 *
 *  Side effects:
 *    A bunch of worlds may be woken up. 
 *    COS will get a notification.
 *
 *----------------------------------------------------------------------------
 */

void
Linux_PollBH(void *data)
{
   vmk_CharDevWakePollers(data);
}

/*
 *----------------------------------------------------------------------
 *
 * copy_ {to,from,in} _user --
 *
 *      Copy to/from host from/to vmkernel/host.
 *
 *      For VMvisor, there is no COS.  Any user-level data is located in
 *      a user-world, whose memory is scoped in a different segment and
 *      which, therefore, requires a segment-overridden 'movs'.
 *
 *      user heap of the kernel. So, a memcpy will suffice.
 *
 * Results: 
 *      0 on success, 1 on failure.
 *
 * Side effects:
 *      Blocking call. 
 *
 *----------------------------------------------------------------------
 */

static inline unsigned long
__CopyOut(void *d, const void *s, unsigned long l)
{
   if (VMK_UNLIKELY(l == 0)) {
      return 0;
   }

   if (__IsPtrVMKBuffer(d)) {
      void *vmkBuf = __SanitizeVMKBufferPtr(d);
      if (vmk_ContextGetCurrentType() == VMK_CONTEXT_TYPE_WORLD) {
         struct umem *umem;
	 struct task_struct *tp;

	 tp = vmklnx_GetCurrent();
	 umem = tp->umem;
         if (umem == NULL) {
            VMKLNX_WARN("Destination address=%p looks invalid\n",d);
            return 1;
         }
	 if ((vmk_VA) vmkBuf >= (vmk_VA) &umem->data[0] &&
	     (vmk_VA) vmkBuf + l <= (vmk_VA) &umem->data[umem->len]) {
	    memcpy(vmkBuf, s, l);
	    return 0;
	 }
      }
   }
   return (vmk_CopyToUser((vmk_VA)d, (vmk_VA)s, l) == VMK_OK)? 0 : 1;
}

static inline unsigned long
__CopyIn(void *d, const void *s, unsigned long l)
{
   if (VMK_UNLIKELY(l == 0)) {
      return 0;
   }

   if (__IsPtrVMKBuffer(s)) {
      void *vmkBuf = __SanitizeVMKBufferPtr(s);
      if (vmk_ContextGetCurrentType() == VMK_CONTEXT_TYPE_WORLD) {
         struct umem *umem;
	 struct task_struct *tp;

	 tp = vmklnx_GetCurrent();
	 umem = tp->umem;
         if (umem == NULL) {
            VMKLNX_WARN("Source address=%p looks invalid\n",s);
            return 1;
         }

	 if ((vmk_VA) vmkBuf >= (vmk_VA) &umem->data[0] &&
	     (vmk_VA) vmkBuf + l <= (vmk_VA) &umem->data[umem->len]) {
	    memcpy(d, vmkBuf, l);
	    return 0;
	 }
      }
   }
   return (vmk_CopyFromUser((vmk_VA)d, (vmk_VA)s, l) == VMK_OK)? 0 : 1;
}

/**                                          
 *  copy_to_user - copy a block of data into user space.
 *  @d: destination address, in user space.
 *  @s: source address, in kernel space.
 *  @len: number of bytes to copy.
 *                                           
 *  Copy data from kernel space to user space.
 *  User context only. This function may sleep.             
 *                                           
 *  ESX Deviation Notes:
 *  Unlike Linux, this function does not do partial copy. The amount of data that 
 *  will be copied should be either all or nothing.  On failure, the content of the 
 *  destination buffer is undefined.
 *
 *  RETURN VALUE:
 *  0 on success, @len on error
 *                                           
 */                                          
/* _VMKLNX_CODECHECK_: copy_to_user */
unsigned long
copy_to_user(void *d, const void *s, unsigned long len)
{
   VMK_ASSERT(vmk_PreemptionIsEnabled() == VMK_FALSE);
   VMKLNX_DEBUG(5, "world=%d from=%p to=%p bytes=%lu", vmk_WorldGetID(),
                s, d, len); 

   return __CopyOut(d, s, len) ? len : 0;
}
EXPORT_SYMBOL(copy_to_user);

/**                                          
 *  copy_from_user - copy a block of data from user space
 *  @d: destination address, in kernel space.
 *  @s: source address, in user space.
 *  @len: number of bytes to copy.
 *                                           
 *  Copy data from user space to kernel space.
 *  User context only. This function may sleep.
 *                                           
 *  ESX Deviation Notes:
 *  Unlike Linux, this function does not do partial copy. The amount of data that 
 *  will be copied should be either all or nothing.  On failure, the content of the 
 *  destination buffer is undefined.
 *
 *  RETURN VALUE:     
 *  0 on success, @len on error
 *                                           
 */                                          
/* _VMKLNX_CODECHECK_: copy_from_user */
unsigned long 
copy_from_user(void *d, const void *s, unsigned long len)
{
   VMK_ASSERT(vmk_PreemptionIsEnabled() == VMK_FALSE);
   VMKLNX_DEBUG(5, "world=%d from=%p to=%p bytes=%lu", vmk_WorldGetID(),
               s, d, len);

   return __CopyIn(d, s, len) ? len : 0;
}
EXPORT_SYMBOL(copy_from_user);

/**                                          
 *  copy_in_user - Copy a block from one user location to another.
 *  @d: Pointer to destination address
 *  @s: Pointer to source address
 *  @len: Number of bytes to copy
 *                                           
 *  Copy the data from user space location s to
 *  to user location d.
 *
 *  ESX Deviation Notes:
 *  This function can fail due to lack of kernel resources.
 *                                           
 *  RETURN VALUE:
 *  0 on success, @len on error
 *                                           
 */                                          
/* _VMKLNX_CODECHECK_: copy_in_user */
unsigned long 
copy_in_user(void *d, const void *s, unsigned long len)
{
   unsigned long ret;
   void *ks;

   VMK_ASSERT(vmk_PreemptionIsEnabled() == VMK_FALSE);
   VMKLNX_DEBUG(5, "world=%d from=%p to=%p bytes=%lu", vmk_WorldGetID(),
                s, d, len);

   ks = vmklnx_kmalloc(VMK_MODULE_HEAP_ID, len, 0, 0);
   if (ks == NULL) {
      VMKLNX_DEBUG(3, "worldId = %d vmk_kmalloc failed", vmk_WorldGetID());
      return len;
   }
   if (__CopyIn(ks, s, len) == 0 && __CopyOut(d, ks, len) == 0) {
      ret = 0;
   } else {
      ret = len;
   }
   vmklnx_kfree(VMK_MODULE_HEAP_ID, ks);
   return ret;
}
EXPORT_SYMBOL(copy_in_user);

/**
 * clear_user - write 0 into user space.
 * @mem: pointer of __user
 * @len: number of bytes to be zeroed
 *
 * Write 0 into user space.
 *
 * RETURN VALUE:
 * 0 on success, @len on failure.
 *
 */
/* _VMKLNX_CODECHECK_: clear_user */
unsigned long
clear_user(void __user *mem, unsigned long len)
{
   char *cmem = mem;

   VMK_ASSERT(vmk_PreemptionIsEnabled() == VMK_FALSE);
   /*
    * 64 bytes of nulls is large enough as a source
    * of zeros in a __CopyOut() call to clear most
    * user buffers that are used as argument to ioctl
    * commands, but also small enough as a local array
    * on a kernel stack.
    */
   char nulls[64];
   unsigned long nr = len;

   VMKLNX_DEBUG(5, "world=%d mem=%p bytes=%lu", vmk_WorldGetID(), mem, nr);

   memset(nulls, 0, sizeof(nulls));

   while (nr) {
      unsigned long nbytes = min_t(unsigned long, nr, sizeof(nulls));

      if (__CopyOut(cmem, nulls, nbytes) != 0) {
         return len;
      }
      cmem += nbytes;
      nr -= nbytes;
   }

  return 0;
}
EXPORT_SYMBOL(clear_user);

void console_print(const char *str)
{
   VMKLNX_INFO("%s", (char *)str);
}

static vmk_Spinlock nbLock;
static struct notifier_block *nb_head;
static vmk_Bool nbNotifiersRunning = VMK_FALSE;

/*
 *----------------------------------------------------------------------
 *
 * register_reboot_notifier --
 *
 *      Register a reboot notifier
 *
 * Results: 
 *      0 on success, -1 on failure.
 *
 * Side effects:
 *      None. 
 *
 *----------------------------------------------------------------------
 */
/**                                          
 *  register_reboot_notifier - Register function to be called at reboot time
 *  @nb: Info about notifier function to be called
 *
 *  Registers a function with the list of functions to be called at reboot time
 *  Always returns 0.
 *
 */
/* _VMKLNX_CODECHECK_: register_reboot_notifier */
int register_reboot_notifier(struct notifier_block *nb)
{
   VMK_ASSERT(vmk_PreemptionIsEnabled() == VMK_FALSE);
   VMKLNX_DEBUG(0, "register reboot notifier %p", nb->notifier_call);
   VMK_ASSERT(nb->notifier_call != NULL);
   nb->modID = vmk_ModuleStackTop();
   vmk_SPLock(&nbLock);
   if (nbNotifiersRunning) {
      vmk_SPUnlock(&nbLock);
      return 0;
   }
   nb->next = nb_head;
   nb_head = nb;
   vmk_SPUnlock(&nbLock);
   return 0;
}
EXPORT_SYMBOL(register_reboot_notifier);

/*
 *----------------------------------------------------------------------
 *
 * unregister_reboot_notifier --
 *
 *      Unregister a reboot notifier
 *
 * Results: 
 *      0 on success, -1 on failure.
 *
 * Side effects:
 *      None. 
 *
 *----------------------------------------------------------------------
 */
/**                                          
 *  unregister_reboot_notifier - Unregisters a previously registered reboot notifier function. 
 *  @nb: pointer to notifier_block
 *                                           
 *  Return value: 
 *   0 on success, -1 on failure.
 *                                           
 */                                          
/* _VMKLNX_CODECHECK_: unregister_reboot_notifier */
int unregister_reboot_notifier(struct notifier_block *nb)
{
   struct notifier_block *n, *np;

   VMK_ASSERT(vmk_PreemptionIsEnabled() == VMK_FALSE);
   VMKLNX_DEBUG(0, "unregister reboot notifier %p", nb->notifier_call);
   vmk_SPLock(&nbLock);
   if (nbNotifiersRunning) {
      vmk_SPUnlock(&nbLock);
      return 0;
   }
   n = nb_head;
   for (np = NULL; n; n = n->next) {
      if (n == nb) {
         if (np) {
            np->next = nb->next;
         } else {
            nb_head = nb->next;
         }
         vmk_SPUnlock(&nbLock);
         return 0;
      }
      np = n;
   }
   vmk_SPUnlock(&nbLock);
   return -1;
}
EXPORT_SYMBOL(unregister_reboot_notifier);

/*
 *----------------------------------------------------------------------
 *
 * RebootHandler --
 *
 *      Run registered reboot notification handlers.  Also call into
 *      LinuxPCI to run device shutdown methods; on native Linux that
 *      occurs right after the reboot notifiers are run.
 *
 * Results: 
 *      None.
 *
 * Side effects:
 *      None. 
 *
 *----------------------------------------------------------------------
 */
static void
RebootHandler(void *data)
{
   struct notifier_block *nb, *nbn;

   data = data;
   vmk_SPLock(&nbLock);
   /* once we're in here we don't let register/unregister do any work */
   VMK_ASSERT(nbNotifiersRunning == VMK_FALSE);
   nbNotifiersRunning = VMK_TRUE;
   nb = nb_head;
   vmk_SPUnlock(&nbLock);
   for (; nb; nb = nbn) {
      nbn = nb->next;
      VMKLNX_DEBUG(0, "running reboot notifier %p", nb->notifier_call);
      VMKAPI_MODULE_CALL_VOID(nb->modID, nb->notifier_call, nb, SYS_POWER_OFF, NULL);
   }
   LinuxPCI_Shutdown();
}

void __brelse(struct buffer_head *bh)
{
   VMKLNX_WARN("__brelse");
   VMKLNX_ASSERT_NOT_IMPLEMENTED(0);
}

void free_dma(unsigned int dmanr)
{
   VMKLNX_WARN("free_dma");
   VMKLNX_ASSERT_NOT_IMPLEMENTED(0);
}

int request_dma(unsigned int dmanr, const char * device_id)
{
   VMKLNX_WARN("request_dma");
   VMKLNX_ASSERT_NOT_IMPLEMENTED(0);
   return 1;
}

static int
LinuxPrintk(const char * fmt, va_list args)
{
   va_list argsCopy;
   int printedLen;

   va_copy(argsCopy, args);

   if (unlikely(fmt != NULL && !strncmp(fmt, VMKLNX_KERN_ALERT, sizeof(VMKLNX_KERN_ALERT) - 1))) {
      /*
       * If fmt begins with VMKLNX_KERN_ALERT, print the message to ESXi console as well.
       */
      vmk_vLogNoLevel(VMK_LOG_URGENCY_ALERT, fmt, args);
   } else {
      vmk_vLogNoLevel(VMK_LOG_URGENCY_NORMAL, fmt, args);
   }
   printedLen = vmk_Vsnprintf(NULL, 0, fmt, argsCopy);

   va_end(argsCopy);
   va_end(args);

   return printedLen;
}

/**
 *  printk - print messages to the vmkernel log
 *  @fmt: the format string
 *
 *  printk is used to print a formatted message to the vmkernel logs from inside
 *  the drivers. 
 *
 *  ESX Deviation Notes:
 *  The priorities for Linux priority code string like KERN_DEBUG etc might be
 *  ignored and the messages will always have the same priority.
 *
 *  RETURN VALUE:
 *  Returns the number of characters written to the log.
 *
 *  SEE ALSO:
 *  printf
 *
 */
/* _VMKLNX_CODECHECK_: printk */
asmlinkage int printk(const char * fmt, ...)
{
   va_list args;
   VMK_ASSERT(vmk_PreemptionIsEnabled() == VMK_FALSE);
   va_start(args, fmt);
   return LinuxPrintk(fmt, args);
}
EXPORT_SYMBOL(printk);

/**
 *  printf - print messages to the vmkernel log
 *  @fmt: the format string
 *
 *  Prints a formatted message to the vmkernel logs. This function is an alias to
 *  printk().
 *
 *  ESX Deviation Notes:
 *  The priorities for Linux priority code string like KERN_DEBUG etc might be
 *  ignored and the messages will always have the same priority.
 *
 *  RETURN VALUE:
 *  Returns the number of characters written to the log.
 *
 *  SEE ALSO:
 *  printk
 *
 */
/* _VMKLNX_CODECHECK_: printf */
asmlinkage int printf(const char * fmt, ...) __attribute__ ((alias ("printk") ));

/**
 *  scnprintf - format a string and place it in a buffer       
 *  @buf: the buffer to place the result into    
 *  @size: the size of the buffer, including the trailing null space
 *  @fmt: the format string to use
 *  @...: arguments for the format string
 *
 *  Format a string and place it in a buffer
 *
 *  RETURN VALUE:
 *  The return value is the number of characters written into @buf not including
 *  the trailing '\0'. If @size is <= 0 the function returns 0. If the return is
 *  greater than or equal to @size, the resulting string is truncated.
 */
/* _VMKLNX_CODECHECK_: scnprintf */
int scnprintf(char * buf, size_t size, const char *fmt, ...)
{
	va_list args;
	int i;

	VMK_ASSERT(vmk_PreemptionIsEnabled() == VMK_FALSE);
	va_start(args, fmt);
	i = vmk_Vsnprintf(buf, size, fmt, args);
	va_end(args);
	return (i >= size) ? (size - 1) : i;
}
EXPORT_SYMBOL(scnprintf);

void * best_memcpy(void * to, const void * from, size_t n)
{
   return memcpy(to, from, n);
}

void * best_memset(void * s, char c, size_t count)
{
   return memset(s, c, count);
}

void
Linux_BH(void (*routine)(void *), void *data) 
{
   vmk_ModuleID modID;

   modID = vmk_ModuleStackTop();
   VMK_ASSERT(modID != VMK_INVALID_MODULE_ID);
   Linux_BHInternal(routine, data, modID);
}

LinuxBHData *
Linux_GetBHList(void)
{
   return vmkLinuxPCPU[vmk_GetPCPUNum()].linuxBHList;
}

void
Linux_SetBHList(LinuxBHData *list)
{
   vmkLinuxPCPU[vmk_GetPCPUNum()].linuxBHList = list;
}

void
Linux_BHInternal(void (*routine)(void *), void *data, vmk_ModuleID modID) 
{
   LinuxBHData *d = (LinuxBHData *)vmk_HeapAlloc(VMK_MODULE_HEAP_ID, sizeof(LinuxBHData));

/*   ASSERT_NO_INTERRUPTS(); */

   VMK_ASSERT(vmk_PreemptionIsEnabled() == VMK_FALSE);
   if (d == NULL) {
      VMKLNX_WARN("Couldn't allocate memory");
   } else {
      vmk_CPUFlags eflags;

      eflags = vmk_CPUGetFlags();
      vmk_CPUDisableInterrupts();
      d->routine = routine;
      d->data = data;
      d->next = Linux_GetBHList();
      d->modID = modID;
      d->staticAlloc = VMK_FALSE;
      Linux_SetBHList(d);
      // schedule Linux_BHHandler for this PCPU
      vmk_BHSchedulePCPU(linuxBHNum, vmk_GetPCPUNum());
      vmk_CPUSetFlags(eflags);
   }
}

void
Linux_BHInternal_static(LinuxBHData *d, vmk_ModuleID modID)
{
   vmk_CPUFlags eflags;

   VMK_ASSERT(vmk_PreemptionIsEnabled() == VMK_FALSE);
   VMK_ASSERT(d);
/*   ASSERT_NO_INTERRUPTS(); */
   VMK_ASSERT(d->routine);
   VMK_ASSERT(d->staticAlloc);

   eflags = vmk_CPUGetFlags();
   vmk_CPUDisableInterrupts();
   d->next = Linux_GetBHList();
   d->modID = modID;
   Linux_SetBHList(d);
   // schedule Linux_BHHandler for this PCPU 
   vmk_BHSchedulePCPU(linuxBHNum, vmk_GetPCPUNum());
   vmk_CPUSetFlags(eflags);
}

void
Linux_BH_static(void (*routine)(void *), void *linuxBHdata) 
{
   vmk_ModuleID modID;

   modID = vmk_ModuleStackTop();
   VMK_ASSERT(modID != VMK_INVALID_MODULE_ID);
   Linux_BHInternal_static((LinuxBHData *)linuxBHdata, modID);
}

void
Linux_BHHandler(void *clientData)
{
   LinuxBHData *d;

   VMK_ASSERT(vmk_PreemptionIsEnabled() == VMK_FALSE);
   if (clientData != LINUX_BHHANDLER_NO_IRQS) {
      VMK_ASSERT_CPU_HAS_INTS_ENABLED();
      vmk_CPUDisableInterrupts();
   }
   d = Linux_GetBHList();
   Linux_SetBHList(NULL);
   if (clientData != LINUX_BHHANDLER_NO_IRQS) {
      vmk_CPUEnableInterrupts();
   }
   while (d != NULL) {
      LinuxBHData *next = d->next;
      VMKAPI_MODULE_CALL_VOID(d->modID, d->routine, d->data);
      if (!d->staticAlloc) {
         vmk_HeapFree(VMK_MODULE_HEAP_ID, d);
      }
      d = next;
   }
}


/**
 * do_gettimeofday - Gets the time in seconds since 1970
 * @tv: pointer to struct timeval where the time is returned
 *
 * Fills in the timeval struct with time in seconds since 1970.
 *
 * RETURN VALUE:
 * NONE
 *
 */
/* _VMKLNX_CODECHECK_: do_gettimeofday */
void
do_gettimeofday(struct timeval *tv)
{
   vmk_TimeVal vmktv;

   VMK_ASSERT(vmk_PreemptionIsEnabled() == VMK_FALSE);

   vmk_GetTimeOfDay(&vmktv);
   tv->tv_sec = vmktv.sec;
   tv->tv_usec = vmktv.usec;
}
EXPORT_SYMBOL(do_gettimeofday);

/*
 * Simulate gettimeofday using do_gettimeofday which only allows a timeval
 * and therefore only yields usec accuracy
 */
void getnstimeofday(struct timespec *tv)
{
	struct timeval x;

	VMK_ASSERT(vmk_PreemptionIsEnabled() == VMK_FALSE);
	do_gettimeofday(&x);
	tv->tv_sec = x.tv_sec;
	tv->tv_nsec = x.tv_usec * NSEC_PER_USEC;
}
EXPORT_SYMBOL_GPL(getnstimeofday);

// return value is only used as argument to probe_irq_off
unsigned long probe_irq_on(void)
{
   return 0x12345678;
}

/* Should return the irq that received ints between irq_on and irq_off,
 * but we only return 0, and ide-probe manually sets the IRQ value in
 * try_to_identify(line 243) */
int probe_irq_off(unsigned long unused)
{
   return 0;
}

/* These functions are supposed to enable and disable hard_idle:hlt,
 * noop for vmkernel */
void disable_hlt(void)
{
}
void enable_hlt(void)
{
}

/**
 *	get_option - Parse integer from an option string
 *	@str: option string
 *	@pint: (output) integer value parsed from @str
 *
 *	Read an int from an option string; if available accept a subsequent
 *	comma as well.
 *
 *	Return values:
 *	0 : no int in string
 *	1 : int found, no subsequent comma
 *	2 : int found including a subsequent comma
 */

int get_option (char **str, int *pint)
{
	char *cur = *str;

	if (!cur || !(*cur))
		return 0;
	*pint = simple_strtol (cur, str, 0);
	if (cur == *str)
		return 0;
	if (**str == ',') {
		(*str)++;
		return 2;
	}

	return 1;
}

/**
 *	get_options - Parse a string into a list of integers
 *	@str: String to be parsed
 *	@nints: size of integer array
 *	@ints: integer array
 *
 *	This function parses a string containing a comma-separated
 *	list of integers.  The parse halts when the array is
 *	full, or when no more numbers can be retrieved from the
 *	string.
 *
 *	Return value is the character in the string which caused
 *	the parse to end (typically a null terminator, if @str is
 *	completely parseable).
 */
 
char *get_options (const char *str, int nints, int *ints)
{
	int res, i = 1;
	char *_str = (char *)str;

	while (i < nints) {
		res = get_option (&_str, ints + i);
		if (res == 0)
			break;
		i++;
		if (res == 1)
			break;
	}
	ints[0] = i - 1;
	return (_str);
}


void do_BUG(const char* file, int line)
{
   vmk_Panic("do_BUG: %s:%d", file, line);         
}


/*
 * Wrapper to conform to the function prototype expected by Linux_BH.
 */
static void
Linux_Softirq(void *unused)
{
   do_softirq();
}

/*
 *----------------------------------------------------------------------
 *
 * init_vmkLinuxPCPU --
 *
 *     initialize the structures in vmkLinuxPCPU for PCPUs that are 
 *     present.
 *
 * Results:
 *	None
 *
 * Side effects:
 *	None
 *
 *----------------------------------------------------------------------
 */
static VMK_ReturnStatus
init_vmkLinuxPCPU(uint32_t numPCPUs)
{
   int i;

   // if the following ASSERT fires, adjust PER_PCPU_VMKLINUX_DATA_CACHE_LINES
   VMK_ASSERT_ON_COMPILE(sizeof(vmkLinuxPCPU_t) 
                         == (PER_PCPU_VMKLINUX_DATA_CACHE_LINES*L1_CACHE_BYTES));
   VMK_ASSERT(numPCPUs == smp_num_cpus);

   vmkLinuxPCPU = vmk_HeapAlign(VMK_MODULE_HEAP_ID,
                                sizeof(vmkLinuxPCPU_t) * numPCPUs,
                                L1_CACHE_BYTES);
   if (vmkLinuxPCPU == NULL) {
      return VMK_NO_MEMORY;
   }
   memset(vmkLinuxPCPU, 0, sizeof(vmkLinuxPCPU_t) * numPCPUs);

   VMKLNX_DEBUG(0, "vmk_NumPCPUs %d &vmkLinuxPCPU[0]: %p; &vmkLinuxPCPU[1]: %p", 
       numPCPUs, &vmkLinuxPCPU[0], &vmkLinuxPCPU[1]);
       
   // double check cache-line alignment
   VMK_ASSERT((((vmk_VA)&vmkLinuxPCPU[0]) % L1_CACHE_BYTES) == 0);
   VMK_ASSERT((((vmk_VA)&vmkLinuxPCPU[1]) % L1_CACHE_BYTES) == 0);

   if ((((vmk_VA)&vmkLinuxPCPU[0]) % L1_CACHE_BYTES) != 0) {
      VMKLNX_WARN("vmkLinuxPCPU[0] %p not cache-line boundary", &vmkLinuxPCPU[0]);
   }
   if ((((vmk_VA)&vmkLinuxPCPU[1]) % L1_CACHE_BYTES) != 0) {
      VMKLNX_WARN("vmkLinuxPCPU[1] %p not cache-line boundary", &vmkLinuxPCPU[1]);
   }

   // now initialize the softirqLinuxBHData data structure.
   for (i = 0; i < numPCPUs; i++) {
      vmkLinuxPCPU[i].softirqLinuxBHData.routine = Linux_Softirq;
      vmkLinuxPCPU[i].softirqLinuxBHData.data = NULL;
      // modID is updated dynamically.
      vmkLinuxPCPU[i].softirqLinuxBHData.modID = VMK_INVALID_MODULE_ID;
      vmkLinuxPCPU[i].softirqLinuxBHData.staticAlloc = VMK_TRUE;
   }

   return VMK_OK;
}

static void
cleanup_vmkLinuxPCPU(void)
{
   vmk_HeapFree(VMK_MODULE_HEAP_ID, vmkLinuxPCPU);
}

// Keep these at the end 
int
init_module(void)
{
   VMK_ReturnStatus status;
   vmk_LogProperties logProps;
   vmk_HeapCreateProps anyHeapProps;
   vmk_HeapCreateProps lowHeapProps;
   vmk_HeapCreateProps emergencyHeapProps;
   vmk_MemPoolProps mem_pool_props;
   vmk_BHProps bhProps;

   vmklinuxModID = vmklnx_this_module_id;
   
   status = vmk_NameInitialize(&logProps.name, VMKLINUX_NAME);
   VMK_ASSERT(status == VMK_OK);
   logProps.module = vmklinuxModID;
   logProps.heap = VMK_MODULE_HEAP_ID;
   logProps.defaultLevel = 0;
   logProps.throttle = NULL;
   status = vmk_LogRegister(&logProps, &vmklinuxLog);                                          
   if (status != VMK_OK) {
      vmk_WarningMessage("vmklinux: init_module: vmk_LogRegister failed: %s", 
                         vmk_StatusToString(status));
      goto log_register_error;
   }

   vmk_LogSetCurrentLogLevel(vmklinuxLog, 1);
   VMKLNX_INFO("vmklinux module load starting...");
   VMKLNX_INFO("Module Version = %s-%s",
                VMKLNX_STRINGIFY(PRODUCT_VERSION),
                VMKLNX_MY_NAMESPACE_VERSION);

   max_pfn = vmk_MA2MPN(vmk_MachMemMaxAddr());
   VMKLNX_INFO("max_pfn = 0x%lx", max_pfn);

   lowHeapProps.type = VMK_HEAP_TYPE_CUSTOM;
   status = vmk_NameInitialize(&lowHeapProps.name, VMKLNX_LOW_HEAP);
   VMK_ASSERT(status == VMK_OK);
   lowHeapProps.module = THIS_MODULE->moduleID;
   lowHeapProps.initial = vmklnx_low_heap_min;
   lowHeapProps.max = vmklnx_low_heap_max;
   lowHeapProps.creationTimeoutMS = VMK_TIMEOUT_NONBLOCKING;
   lowHeapProps.typeSpecific.custom.physContiguity = VMK_MEM_PHYS_CONTIGUOUS;
   lowHeapProps.typeSpecific.custom.physRange = VMK_PHYS_ADDR_BELOW_4GB;
   
   status = vmk_HeapCreate(&lowHeapProps, &vmklnxLowHeap);
   if (status != VMK_OK) {
      VMKLNX_WARN("vmk_HeapCreate for low heap failed with error: %s",
                  vmk_StatusToString(status));
      goto heap_create_error;
   }
   vmklnx_codma.heapID = vmklnxLowHeap;

   emergencyHeapProps.type = VMK_HEAP_TYPE_CUSTOM;
   status = vmk_NameInitialize(&emergencyHeapProps.name, VMKLNX_EMERGENCY_HEAP);
   VMK_ASSERT(status == VMK_OK);
   emergencyHeapProps.module = THIS_MODULE->moduleID;
   emergencyHeapProps.initial = VMKLNX_EMERGENCY_HEAP_MIN;
   emergencyHeapProps.max = VMKLNX_EMERGENCY_HEAP_MAX;
   emergencyHeapProps.creationTimeoutMS = VMK_TIMEOUT_NONBLOCKING;
   emergencyHeapProps.typeSpecific.custom.physContiguity = VMK_MEM_PHYS_ANY_CONTIGUITY;
   emergencyHeapProps.typeSpecific.custom.physRange = VMK_PHYS_ADDR_ANY;

   status = vmk_HeapCreate(&emergencyHeapProps, &vmklnxEmergencyHeap);
   if (status != VMK_OK) {
      VMKLNX_WARN("vmk_HeapCreate for emergency heap failed with error: %s",
                  vmk_StatusToString(status));
      goto emergency_heap_create_error;
   }

   VMK_ASSERT_ON_COMPILE(HZ == VMK_JIFFIES_PER_SECOND);

   is_vmvisor = VMK_TRUE;

   smp_num_cpus = vmk_NumPCPUs();
   if (init_vmkLinuxPCPU(vmk_NumPCPUs()) != VMK_OK) {
      goto init_pcpus_error;
   }

   status = vmk_SPCreateIRQ_LEGACY(&regionLock, vmklinuxModID, "regionLck", NULL,
                            VMK_SP_RANK_IRQ_MEMTIMER_LEGACY);
   VMK_ASSERT(status == VMK_OK);

   status = vmk_SPCreateIRQ_LEGACY(&irqMappingLock, vmklinuxModID, "irqMapLck", NULL,
                            VMK_SP_RANK_IRQ_MEMTIMER_LEGACY-1);
   VMK_ASSERT(status == VMK_OK);

   status = vmk_SPCreate_LEGACY(&nbLock, vmklinuxModID, "nbLock", NULL,
                         VMK_SP_RANK_LEAF_LEGACY);
   VMK_ASSERT(status == VMK_OK);

   
   status = vmk_SemaCreate(&pci_bus_sem, vmk_ModuleCurrentID,
                           "PCI_BUS_SEMA", 1);
   if (status != VMK_OK) {
      VMKLNX_WARN("vmklinux: init_module: vmk_SemaCreate for pci_bus_sem failed: %s", 
                  vmk_StatusToString(status));
      goto undo_lock_log_init;
   }

   if ((status = vmklnx_mod_init()) != VMK_OK) {
      VMKLNX_WARN("vmklinux: init_module: module list initialization failed %s",
                         vmk_StatusToString(status));
      goto undo_pci_bus_sem;
   }

   if ((status = vmklnx_register_module(THIS_MODULE, THIS_MODULE->moduleID)) !=
                                                                      VMK_OK) {
      VMKLNX_WARN("vmklinux: init_module: adding to module list failed (%#x): %s",
                         status, vmk_StatusToString(status));
      goto undo_vmklnx_mod_init;
   }

   status = vmk_ConfigParamOpen("Disk", "DumpPollMaxRetries",
                                &dumpPollRetriesHandle);
   VMK_ASSERT(status == VMK_OK);
   status = vmk_ConfigParamOpen("Disk", "DumpPollDelay",
                                &dumpPollDelayHandle);
   VMK_ASSERT(status == VMK_OK);

   LinuxEFI_Init();
   LinuxTime_Init();

   if (LinuxWorkQueue_Init() < 0) {
      VMKLNX_WARN("vmklinux: init_module: LinuxWorkQueue_Init failed");
      goto undo_config_param_init;
   }

   platform_bus_init();

   LinuxTask_Init();
   LinuxKthread_Init();
   LinuxProc_Init();
   LinuxPCI_Init();
   LinuxDMA_Init();
   LinNet_Init();
   SCSILinux_Init();
   LinuxCNA_Init();
   LinuxUSB_Init();
   BlockLinux_Init();
   LinuxChar_Init();
   LinuxIRQ_Init();
   if (softirq_init() != VMK_OK) {
      goto undo_bunch_init;
   }

   dmi_scan_machine();

   if (vmk_RegisterRebootHandler(RebootHandler, NULL) != VMK_OK) {
      goto undo_softirq;
   }

   bhProps.moduleID = vmklinuxModID;
   status = vmk_NameInitialize(&bhProps.name, "linuxbh");
   VMK_ASSERT(status == VMK_OK);
   bhProps.callback = Linux_BHHandler;
   bhProps.priv = NULL;
   status = vmk_BHRegister(&bhProps, &linuxBHNum);
   if (status != VMK_OK) {
      goto undo_reboot_register;
   }

   if (input_init()) {
      VMKLNX_WARN("input_init() failed");
      goto undo_bh_register;
   }

   if (hid_init()) {
      VMKLNX_WARN("hid_init() failed");
      goto undo_input_init;
   }

   if (lnx_kbd_init()) {
      VMKLNX_WARN("lnx_kbd_init() failed");
      goto undo_hid_init;
   }

   if (mousedev_init()) {
      VMKLNX_WARN("mousedev_init() failed");
      goto undo_hid_init;
   }

   mem_pool_props.module = THIS_MODULE->moduleID;
   mem_pool_props.parentMemPool = THIS_MODULE->parent_mem_pool;
   mem_pool_props.memPoolType = VMK_MEM_POOL_LEAF;
   mem_pool_props.resourceProps.reservation = 
      VMK_UTIL_ROUNDUP(vmklnx_vmalloc_heap_min, VMK_PAGE_SIZE) / VMK_PAGE_SIZE;
   mem_pool_props.resourceProps.limit = 
      VMK_UTIL_ROUNDUP(vmklnx_vmalloc_heap_max, VMK_PAGE_SIZE) / VMK_PAGE_SIZE;

   status = vmk_NameFormat(&mem_pool_props.name, "%s", VMKLNX_VMALLOC_HEAP);
   VMK_ASSERT(status == VMK_OK);

   status = vmk_MemPoolCreate(&mem_pool_props, &mem_pool);

   if (status != VMK_OK) {
      VMKLNX_WARN("vmk_MemPoolCreate for __vmalloc failed with error: %s",
                  vmk_StatusToString(status));
      /*
       * If we fail to create the memory pool, we will get assigned to a
       * default memory pool instead.
       */
      mem_pool = VMK_MEMPOOL_INVALID;
   }
   VMK_ASSERT(status == VMK_OK);

   status = vmk_NameInitialize(&anyHeapProps.name, VMKLNX_VMALLOC_HEAP);
   VMK_ASSERT(status == VMK_OK);
   anyHeapProps.module = THIS_MODULE->moduleID;
   anyHeapProps.initial = vmklnx_vmalloc_heap_min;
   anyHeapProps.max = vmklnx_vmalloc_heap_max;
   anyHeapProps.type = VMK_HEAP_TYPE_MEMPOOL;
   anyHeapProps.typeSpecific.memPool.physContiguity = VMK_MEM_PHYS_ANY_CONTIGUITY;
   anyHeapProps.typeSpecific.memPool.physRange = VMK_PHYS_ADDR_ANY;
   anyHeapProps.typeSpecific.memPool.memPool = mem_pool;
   anyHeapProps.creationTimeoutMS = VMK_TIMEOUT_NONBLOCKING;

   status = vmk_HeapCreate(&anyHeapProps, &vmklnxVmallocHeap);

   if (status != VMK_OK) {
      VMKLNX_WARN("vmk_HeapCreate for __vmalloc failed with error: %s",
                  vmk_StatusToString(status));
      goto undo_hid_init;
   }

   if (pm_start_workqueue() != 0) {
      goto undo_vmalloc_init;
   }

   VMKLNX_INFO("vmklinux module load successful.");
   return 0;

undo_vmalloc_init:
   vmk_HeapDestroy(vmklnxVmallocHeap);

undo_hid_init:
   if (mem_pool != VMK_MEMPOOL_INVALID) {
      vmk_MemPoolDestroy(mem_pool);
   }
   hid_exit();
undo_input_init:
   input_exit();
undo_bh_register:
   vmk_BHUnregister(linuxBHNum);
undo_reboot_register:
   vmk_UnregisterRebootHandler(RebootHandler);
undo_softirq:
   softirq_cleanup();
undo_bunch_init:
   LinuxPCI_Cleanup();
   LinNet_Cleanup();
   SCSILinux_Cleanup();
   LinuxCNA_Cleanup();
   LinuxUSB_Cleanup();
   BlockLinux_Cleanup();
   LinuxIRQ_Cleanup();
   LinuxChar_Cleanup();
   LinuxProc_Cleanup();
   LinuxKthread_Cleanup();
   LinuxTask_Cleanup();
   LinuxWorkQueue_Cleanup();
   LinuxTime_Cleanup();

undo_config_param_init:
   vmk_ConfigParamClose(dumpPollDelayHandle);
   vmk_ConfigParamClose(dumpPollRetriesHandle);
   vmklnx_unregister_module(THIS_MODULE);

undo_vmklnx_mod_init:
   vmklnx_mod_uninit();

undo_pci_bus_sem:
   vmk_SemaDestroy(&pci_bus_sem);

undo_lock_log_init:
   vmk_SPDestroyIRQ(&irqMappingLock);
   vmk_SPDestroyIRQ(&regionLock);
   vmk_SPDestroy(&nbLock);
   cleanup_vmkLinuxPCPU();

init_pcpus_error:
   vmk_HeapDestroy(vmklnxEmergencyHeap);

emergency_heap_create_error:
   vmk_HeapDestroy(vmklnxLowHeap);

heap_create_error:
   vmk_LogUnregister(vmklinuxLog);

log_register_error:
   return -1;
}

void
cleanup_module(void)
{
   pm_stop_workqueue();
   mousedev_exit();
   hid_exit();
   input_exit();
   // kill kernel thread
   vmk_UnregisterRebootHandler(RebootHandler);

   LinuxPCI_Cleanup();
#if defined(__x86_64__)
   LinuxDMA_Cleanup();
#endif /* defined(__x86_64__) */
   LinNet_Cleanup();
   SCSILinux_Cleanup();
   LinuxCNA_Cleanup();
   BlockLinux_Cleanup();
   vmk_HeapDestroy(vmklnxEmergencyHeap);
   vmk_HeapDestroy(vmklnxLowHeap);
   LinuxIRQ_Cleanup();
   softirq_cleanup();
   LinuxChar_Cleanup();
   LinuxProc_Cleanup();
   LinuxKthread_Cleanup();
   LinuxTask_Cleanup();
   LinuxWorkQueue_Cleanup();
   LinuxTime_Cleanup();

   vmk_SPDestroyIRQ(&irqMappingLock);
   vmk_SPDestroyIRQ(&regionLock);
   vmk_SPDestroy(&nbLock);
   cleanup_vmkLinuxPCPU();
   vmklnx_unregister_module(THIS_MODULE);
   vmklnx_mod_uninit();
   vmk_SemaDestroy(&pci_bus_sem);
   vmk_LogUnregister(vmklinuxLog);
}

/*
 * A BUG() call in an inline function in a header should be avoided,
 * because it can seriously bloat the kernel.  So here we have
 * helper functions.
 * We lose the BUG()-time file-and-line info this way, but it's
 * usually not very useful from an inline anyway.  The backtrace
 * tells us what we want to know.
 */

void __out_of_line_bug(int line)
{
	printk("kernel BUG in header file at line %d\n", line);

	BUG();

	/* Satisfy __attribute__((noreturn)) */
	for ( ; ; )
		;
}

/**                                          
 *  get_random_bytes - Gets the requested number of random bytes
 *  @buf: Specifies the address of the buffer in which the requested bytes are stored
 *  @nbytes: Specifies the number of random bytes
 *                                           
 *  Returns the requested number of random bytes in the supplied buffer.
 *                                           
 *  RETURN VALUE:
 *  NONE 
 */                                          
/* _VMKLNX_CODECHECK_: get_random_bytes */
void 
get_random_bytes(void *buf, int nbytes)
{
   int  bytesReturned;

   VMK_ASSERT(vmk_PreemptionIsEnabled() == VMK_FALSE);

   vmkplxr_GetRandomBytes(buf, nbytes, &bytesReturned);
}
EXPORT_SYMBOL(get_random_bytes);

/**
 *  add_disk_randomness - supply data for the random data source device
 *  @disk: the pointer to gendisk structure
 * 
 *  Supply data for the random data source device
 *      
 *  ESX Deviation Notes:
 *  Implemented as an inline call to function registered by random driver.
 *  Call is protected against case where random driver has not been loaded.
 *
 *  RETURN VALUE:
 *  NONE
 */
/* _VMKLNX_CODECHECK_: add_disk_randomness */
void
add_disk_randomness(struct gendisk *disk)
{
   VMK_ASSERT(vmk_PreemptionIsEnabled() == VMK_FALSE);

   vmkplxr_AddStorageEntropy(disk ? disk->major : 1);
}
EXPORT_SYMBOL(add_disk_randomness);

/**   
 *  add_input_randomness - supply data for the random data source device
 *  @type: enum (EV_KEY for keyboard, EV_REL for mouse)
 *  @code: int (scancode for keyboard, position for mouse)
 *  @value: int (ignored, random driver fakes it)
 *
 *  ESX Deviation Notes:
 *  Implemented as an inline call to functions registered by random driver.
 *  Calls are protected against case where random driver has not been loaded.
 *
 */
/* _VMKLNX_CODECHECK_: add_input_randomness */
void
add_input_randomness(unsigned int type, unsigned int code, unsigned int value)
{
   VMK_ASSERT(vmk_PreemptionIsEnabled() == VMK_FALSE);

   switch (type) {
      case EV_KEY:
         vmkplxr_AddKeyboardEntropy(code);
         break;

      case EV_REL:
         vmkplxr_AddMouseEntropy(code);
         break;

      default:
         // XXX should "never" happen
         break;
   }
}
EXPORT_SYMBOL(add_input_randomness);

VMK_ReturnStatus 
vmklnx_register_random_driver(LinuxRandomDriver *driver)
{
   VmkplxrRandomDriver vmkplxr_driver;

   VMK_ASSERT(vmk_PreemptionIsEnabled() == VMK_FALSE);

   vmkplxr_driver.modID = vmk_ModuleStackTop();
   vmkplxr_driver.addInterruptEntropy = driver->add_interrupt_entropy;
   vmkplxr_driver.addHwrngEntropy = driver->add_hwrng_entropy;
   vmkplxr_driver.addKeyboardEntropy = driver->add_keyboard_entropy;
   vmkplxr_driver.addMouseEntropy = driver->add_mouse_entropy;
   vmkplxr_driver.addStorageEntropy = driver->add_storage_entropy;
   vmkplxr_driver.getHwRandomBytes = driver->get_hw_random_bytes;
   vmkplxr_driver.getHwrngRandomBytes = driver->get_hwrng_random_bytes;
   vmkplxr_driver.getHwRandomBytesNonblocking = driver->get_hw_random_bytes_nonblocking;
   vmkplxr_driver.getSwRandomBytes = driver->get_sw_random_bytes;
   vmkplxr_driver.getSwOnlyRandomBytes = driver->get_sw_only_random_bytes;

   return vmkplxr_RegisterRandomDriver(&vmkplxr_driver);
}
EXPORT_SYMBOL(vmklnx_register_random_driver);

VMK_ReturnStatus 
vmklnx_unregister_random_driver(LinuxRandomDriver *driver)
{
   VmkplxrRandomDriver vmkplxr_driver;

   VMK_ASSERT(vmk_PreemptionIsEnabled() == VMK_FALSE);

   vmkplxr_driver.modID = vmk_ModuleStackTop();
   vmkplxr_driver.addInterruptEntropy = driver->add_interrupt_entropy;
   vmkplxr_driver.addHwrngEntropy = driver->add_hwrng_entropy;
   vmkplxr_driver.addKeyboardEntropy = driver->add_keyboard_entropy;
   vmkplxr_driver.addMouseEntropy = driver->add_mouse_entropy;
   vmkplxr_driver.addStorageEntropy = driver->add_storage_entropy;
   vmkplxr_driver.getHwRandomBytes = driver->get_hw_random_bytes;
   vmkplxr_driver.getHwrngRandomBytes = driver->get_hwrng_random_bytes;
   vmkplxr_driver.getHwRandomBytesNonblocking = driver->get_hw_random_bytes_nonblocking;
   vmkplxr_driver.getSwRandomBytes = driver->get_sw_random_bytes;
   vmkplxr_driver.getSwOnlyRandomBytes = driver->get_sw_only_random_bytes;

   return vmkplxr_UnregisterRandomDriver(&vmkplxr_driver);
}
EXPORT_SYMBOL(vmklnx_unregister_random_driver);

/*
 *----------------------------------------------------------------------
 *
 * vmklnx_get_driver_module_id --
 *
 *      Returns the module id corresponding to the driver if one
 *      exists else returns vmk_ModuleStackTop().
 *
 * Results:
 *      Returns the module id 
 *
 * Side Effects:
 *      None.
 *----------------------------------------------------------------------
 */

vmk_ModuleID
vmklnx_get_driver_module_id(const struct device_driver *drv)
{
   vmk_ModuleID moduleID = VMK_INVALID_MODULE_ID;

   if (likely(drv != NULL)) {
      VMK_ASSERT(drv->owner != NULL);
      if (likely(drv->owner != NULL)) {
         moduleID = drv->owner->moduleID;
      }
      VMK_ASSERT(moduleID != VMK_INVALID_MODULE_ID);
   }

   if (unlikely(moduleID == VMK_INVALID_MODULE_ID)) {
      moduleID = vmk_ModuleStackTop();
   }

   return moduleID;
}

/*
 * This is a simple implementation of vmalloc using a physically discontiguous heap.
 * vmalloc is only supposed to be used for
 * allocations that are large so the overhead shouldn't be that large of a
 * percentage of the overall allocation.  We could save overhead by allocating
 * smaller vmalloc requests off of the heap.
 */
void *
__vmalloc(unsigned long size, gfp_t flags,  pgprot_t prot)
{
   void *addr;

   VMK_ASSERT(vmk_PreemptionIsEnabled() == VMK_FALSE);

   if (size == 0) {
      VMKLNX_WARN("Invalid size argument");
      return NULL;
   }
   addr = vmk_HeapAlloc(vmklnxVmallocHeap, size);
   if (addr == NULL){
      VMKLNX_WARN("Couldn't allocate virtual memory");
      return NULL;
   }
   return addr;
}
EXPORT_SYMBOL(__vmalloc);

/**                                          
 *  vmap - Create a virtually contiguous mapping for a given array of pages       
 *  @pages: Array of pages    
 *  @count: Number of elements in the page array
 *  @flags: Flags for the underlying vm_area created by Linux 
 *  @prot: Page protection for each page of the new mapping
 *
 *  Returns the new virtual address upon success, NULL upon failure.
 *                                           
 *  vmap() takes an array of page structures that may or may not be physically
 *  contiguous and creates a single, new virtually contiguous mapping covering
 *  the pages.  This mapping is valid in the vmklinux kernel-virtual address
 *  space and is in addition to any existing mapping(s) for the pages.
 *  page_to_virt() will return the default virtual mapping for the component 
 *  pages after calling vmap(), *not* the new mapping that was created with 
 *  vmap().  You must keep track of the original mapped address if you want to 
 *  later unmap it.
 *                                           
 *  ESX Deviation Notes:                     
 *  flags and prot are ignored.  This function always implements the behavior
 *  of flags = VM_MAP and prot = PAGE_KERNEL 
 *                                           
 */
/* _VMKLNX_CODECHECK_: vmap */
void *vmap(struct page **pages, unsigned int count, 
		   unsigned long flags, pgprot_t prot)
{
   vmk_MpnRange *ranges;
   vmk_MapRequest mRequest;
   void *vaddr = NULL;
   VMK_ReturnStatus status;
   unsigned int i;
   
   VMK_ASSERT(vmk_PreemptionIsEnabled() == VMK_FALSE);

   ranges = vmalloc(sizeof(vmk_MpnRange) * count);
   if(ranges == NULL) {
      return NULL;
   }
   
   for(i = 0; i < count; i++) {
      ranges[i].numPages = 1;
      ranges[i].startMPN = (vmk_MPN) page_to_pfn(pages[i]);		 
   }
    
   mRequest.mapType = VMK_MAPTYPE_DEFAULT;
   mRequest.mapAttrs = VMK_MAPATTRS_READWRITE;
   mRequest.mpnRanges = &ranges[0];
   mRequest.numElements = count;
   status = vmk_Map(vmklinuxModID, &mRequest, (vmk_VA *)&vaddr);  
   if(status != VMK_OK) {
      vaddr = NULL;
   }

   vfree(ranges);
   return vaddr;
}
EXPORT_SYMBOL(vmap);

/**                                          
 *  vunmap - Unmap a virtually contiguous mapping that was created with vmap
 *  @addr: Virtual address created with vmap    
 *                                           
 *  vunmap() tears down the virtual mapping that was created with vmap().
 *  This function should only be used on virtual mappings that were created
 *  with vmap().
 * 
 */
/* _VMKLNX_CODECHECK_: vunmap */
void vunmap(void *addr)
{
   VMK_ASSERT(vmk_PreemptionIsEnabled() == VMK_FALSE);

   vmk_Unmap((vmk_VA) addr);
}
EXPORT_SYMBOL(vunmap);


/**                                          
 *  vfree - Free the memory
 *  @addr: pointer to memory area to be freed
 *                                           
 *  Frees the memory allocated by the vmalloc().
 *                                           
 *  RETURN VALUE:
 *  None.
 *                                           
 */                                          
/* _VMKLNX_CODECHECK_: vfree */
void 
vfree(void * addr)
{
   VMK_ASSERT(vmk_PreemptionIsEnabled() == VMK_FALSE);

   // To comply with Linux implementation.
   if (addr == NULL) {
      return;
   }
   vmk_HeapFree(vmklnxVmallocHeap, addr);
}
EXPORT_SYMBOL(vfree);

void
Linux_OpenSoftirq(int nr, void (*action)(struct softirq_action *), void *data)
{
   VMK_ASSERT(nr < MAX_SOFTIRQ);

   softirq_vec[nr].action = action;
   softirq_vec[nr].data = data;
}

/**                                          
 *  raw_smp_processor_id - Returns the ID of the current cpu   
 *                                           
 */                                          
/* _VMKLNX_CODECHECK_: raw_smp_processor_id */
uint32_t
raw_smp_processor_id(void)
{
   VMK_ASSERT(vmk_PreemptionIsEnabled() == VMK_FALSE);
   return vmk_GetPCPUNum();
}
EXPORT_SYMBOL(raw_smp_processor_id);

u8 cpu_physical_id(uint32_t cpu)
{
   return vmk_GetAPICCPUID(cpu);
}

static uint32_t
LinuxAtomicReadWriteSoftirqFlag(uint32_t val)
{
   return (uint32_t)vmk_AtomicReadWrite64(&vmkLinuxPCPU[vmk_GetPCPUNum()].
                                          softirq_pending, (vmk_uint64)val);
}

static uint32_t
LinuxAtomicFetchAndOrSoftirqFlag(uint32_t val)
{
   return (uint32_t)vmk_AtomicReadOr64(&vmkLinuxPCPU[vmk_GetPCPUNum()].
                                       softirq_pending, (vmk_uint64)val);
}

unsigned int
softirq_pending(int cpu)
{
   VMK_ASSERT(cpu < vmk_NumPCPUs());
   return (unsigned int)vmk_AtomicRead64(&vmkLinuxPCPU[cpu].softirq_pending);
}

void
__cpu_raise_softirq(int cpu, int nr)
{
   uint32_t pending;
   uint32_t this_cpu = smp_processor_id();

   VMK_ASSERT(vmk_PreemptionIsEnabled() == VMK_FALSE);
   VMK_ASSERT(this_cpu < smp_num_cpus);
   VMK_ASSERT(nr < 32);
   pending = LinuxAtomicFetchAndOrSoftirqFlag((1 << nr));

   // see if need to schedule a softirq on this cpu.
   if (!pending) {
      Linux_BH_static(Linux_Softirq, &vmkLinuxPCPU[this_cpu].softirqLinuxBHData);
   }
}
EXPORT_SYMBOL(__cpu_raise_softirq);

void __attribute__((regparm(3)))
cpu_raise_softirq(unsigned int cpu, unsigned int nr)
{
   __cpu_raise_softirq(cpu, nr);

   /*
    * XXX we should look into creating some kernel thread like
    * linux does with softirqd (see their impl of this fn)
    */
}

/* TODO: dilpreet Needs to be revisited */
void
__raise_softirq_irqoff(unsigned int nr)
{
   uint32_t pending;
   uint32_t this_cpu = smp_processor_id();

   VMK_ASSERT(this_cpu < smp_num_cpus);
   VMK_ASSERT(nr < 32);
   pending = LinuxAtomicFetchAndOrSoftirqFlag((1 << nr));

   // see if need to schedule a softirq on this cpu.
   if (!pending) {
      Linux_BH_static(Linux_Softirq, &vmkLinuxPCPU[this_cpu].softirqLinuxBHData);
   }
}

void fastcall
raise_softirq_irqoff(unsigned int nr)
{
   __raise_softirq_irqoff(nr);
}

void fastcall
raise_softirq(unsigned int nr)
{
   raise_softirq_irqoff(nr);
}

asmlinkage void do_softirq(void)
{
   uint32_t pending;
   int i;

   pending = LinuxAtomicReadWriteSoftirqFlag(0);
   for (i = 0; i < MAX_SOFTIRQ; i++) {
      if (pending & (1 << i)) {
         VMK_ASSERT(softirq_vec[i].action);
         softirq_vec[i].action(softirq_vec[i].data);
      }
   }
}


void
vmklnx_pollwait(struct file *filp,
           wait_queue_head_t *q,
           poll_table *token)
{
   unsigned long flags;
   VMK_ASSERT(vmk_PreemptionIsEnabled() == VMK_FALSE);

   vmk_CharDevSetPollContext((vmk_PollContext *)token, (vmk_PollToken *)q);

   spin_lock_irqsave(&q->lock, flags);
   q->wakeupType = WAIT_QUEUE_POLL;
   spin_unlock_irqrestore(&q->lock, flags);
}
EXPORT_SYMBOL(vmklnx_pollwait);

#ifdef VM_X86_64
/*
 * XXX64: This needs to be investigated further. Seems like its going to be
 * deprecated in future.
 */
int
register_ioctl32_conversion(unsigned int cmd,
			    void *handler)
{
   return 0;
}

int
unregister_ioctl32_conversion(unsigned int cmd)
{
   return 0;
}

#endif

int request_firmware(const struct firmware **firmware_p, const char *name, 
		     struct device *device)
{
   VMK_ASSERT(vmk_PreemptionIsEnabled() == VMK_FALSE);
   VMKLNX_WARN("request_firmware");
   VMKLNX_ASSERT_NOT_IMPLEMENTED(0);
   return 0;
}
EXPORT_SYMBOL(request_firmware);

/**                                          
 *  release_firmware - release the resource associated with a firmware image
 *  @fw: firmware resource to release
 *                                           
 *  Release the resource associated with a firmware image. This call is
 *  unimplemented in ESX and raises an ASSERT in developer builds.
 *
 *  ESX Deviation Notes:                     
 *  NOT IMPLEMENTED.
 *                                           
 */                                          
/* _VMKLNX_CODECHECK_: release_firmware */
void release_firmware(const struct firmware *fw)
{
   VMK_ASSERT(vmk_PreemptionIsEnabled() == VMK_FALSE);
   VMKLNX_WARN("release_firmware");
   VMKLNX_ASSERT_NOT_IMPLEMENTED(0);
}
EXPORT_SYMBOL(release_firmware);

/*
 * printk rate limiting, lifted from the networking subsystem.
 *
 * This enforces a rate limit: not more than one kernel message
 * every printk_ratelimit_jiffies to make a denial-of-service
 * attack impossible.
 */
int __printk_ratelimit(int ratelimit_jiffies, int ratelimit_burst)
{
	static DEFINE_SPINLOCK(ratelimit_lock);
	static unsigned long toks = 10 * 5 * HZ;
	static unsigned long last_msg;
	static int missed;
	unsigned long flags;
	unsigned long now = jiffies;

	spin_lock_irqsave(&ratelimit_lock, flags);
	toks += now - last_msg;
	last_msg = now;
	if (toks > (ratelimit_burst * ratelimit_jiffies))
		toks = ratelimit_burst * ratelimit_jiffies;
	if (toks >= ratelimit_jiffies) {
		int lost = missed;

		missed = 0;
		toks -= ratelimit_jiffies;
		spin_unlock_irqrestore(&ratelimit_lock, flags);
		if (lost)
			printk(KERN_WARNING "printk: %d messages suppressed.\n", lost);
		return 1;
	}
	missed++;
	spin_unlock_irqrestore(&ratelimit_lock, flags);
	return 0;
}

/**
 * printk_ratelimit - guard against excessive message logging
 *
 * Restricts message logging rate not to exceed one every 5 seconds.
 * Here is a typical usage
 *
 * 	if (printk_ratelimit()) printk(...);
 *
 * RETURN VALUE:
 * 1 if rate limit has not been exceeded; otherwise 0.
 *
 */
/* _VMKLNX_CODECHECK_: printk_ratelimit */
int printk_ratelimit(void)
{
	VMK_ASSERT(vmk_PreemptionIsEnabled() == VMK_FALSE);
	return __printk_ratelimit(printk_ratelimit_jiffies,
				printk_ratelimit_burst);
}
EXPORT_SYMBOL(printk_ratelimit);

int net_ratelimit(void)
{
	VMK_ASSERT(vmk_PreemptionIsEnabled() == VMK_FALSE);
	return __printk_ratelimit(net_msg_cost, net_msg_burst);
}
EXPORT_SYMBOL(net_ratelimit);

/* from kernel/resource.c for platform.c */
#ifdef read_lock_irqsave
#undef read_lock_irqsave
#endif

#ifdef read_unlock_irqrestore
#undef read_unlock_irqrestore
#endif

#ifdef write_lock_irqsave
#undef write_lock_irqsave
#endif

#ifdef write_unlock_irqrestore
#undef write_unlock_irqrestore
#endif

#define read_lock_irqsave(_lck, _flags) spin_lock_irqsave(_lck, _flags)
#define read_unlock_irqrestore(_lck, _flags) spin_unlock_irqrestore(_lck, _flags)
#define write_lock_irqsave(_lck, _flags) spin_lock_irqsave(_lck, _flags)
#define write_unlock_irqrestore(_lck, _flags) spin_unlock_irqrestore(_lck, _flags)

#define rwlock_t spinlock_t
#ifdef rwlock_init
#undef rwlock_init
#endif
#define rwlock_init(_lck) spin_lock_init(_lck)

static DEFINE_RWLOCK(resource_lock);

static int __release_resource(struct resource *old)
{
        struct resource *tmp, **p;

        p = &old->parent->child;
        for (;;) {
                tmp = *p;
                if (!tmp)
                        break;
                if (tmp == old) {
                        *p = tmp->sibling;
                        old->parent = NULL;
                        return 0;
                }
                p = &tmp->sibling;
        }
        return -EINVAL;
}

/**
 * release_resource - release a previously reserved resource
 * @old: resource pointer
 */
int release_resource(struct resource *old)
{
        int retval;
#if defined(__VMKLNX__)
        unsigned long flags;

        write_lock_irqsave(&resource_lock, flags);
#else
        write_lock(&resource_lock);
#endif
        retval = __release_resource(old);
#if defined(__VMKLNX__)
        write_unlock_irqrestore(&resource_lock, flags);
#else
        write_unlock(&resource_lock);
#endif
        return retval;
}

/* Return the conflict entry if you can't request it */
static struct resource * __request_resource(struct resource *root, struct resource *new)
{
        resource_size_t start = new->start;
        resource_size_t end = new->end;
        struct resource *tmp, **p;

        if (end < start)
                return root;
        if (start < root->start)
                return root;
        if (end > root->end)
                return root;
        p = &root->child;
        for (;;) {
                tmp = *p;
                if (!tmp || tmp->start > end) {
                        new->sibling = tmp;
                        *p = new;
                        new->parent = root;
                        return NULL;
                }
                p = &tmp->sibling;
                if (tmp->end < start)
                        continue;
                return tmp;
        }
}

/**
 * insert_resource - Inserts a resource in the resource tree
 * @parent: parent of the new resource
 * @new: new resource to insert
 *
 * Returns 0 on success, -EBUSY if the resource can't be inserted.
 *
 * This function is equivalent to request_resource when no conflict
 * happens. If a conflict happens, and the conflicting resources
 * entirely fit within the range of the new resource, then the new
 * resource is inserted and the conflicting resources become children of
 * the new resource.
 */
int insert_resource(struct resource *parent, struct resource *new)
{
        int result;
        struct resource *first, *next;
#if defined(__VMKLNX__)
        unsigned long flags;

        write_lock_irqsave(&resource_lock, flags);
#else
        write_lock(&resource_lock);
#endif

        for (;; parent = first) {
                result = 0;
                first = __request_resource(parent, new);
                if (!first)
                        goto out;

                result = -EBUSY;
                if (first == parent)
                        goto out;

                if ((first->start > new->start) || (first->end < new->end))
                        break;
                if ((first->start == new->start) && (first->end == new->end))
                        break;
        }

        for (next = first; ; next = next->sibling) {
                /* Partial overlap? Bad, and unfixable */
                if (next->start < new->start || next->end > new->end)
                        goto out;
                if (!next->sibling)
                        break;
                if (next->sibling->start > new->end)
                        break;
        }

        result = 0;

        new->parent = parent;
        new->sibling = next->sibling;
        new->child = first;

        next->sibling = NULL;
        for (next = first; next; next = next->sibling)
                next->parent = new;

        if (parent->child == first) {
                parent->child = new;
        } else {
                next = parent->child;
                while (next->sibling != first)
                        next = next->sibling;
                next->sibling = new;
        }

 out:
#if defined(__VMKLNX__)
        write_unlock_irqrestore(&resource_lock, flags);
#else
        write_unlock(&resource_lock);
#endif
        return result;
}

vmklnx_kbd_handle
vmklnx_register_usb_kbd_int_handler(vmklnx_usb_interrupt_info *int_info)
{
   vmk_KeyboardInterruptHandle handle;
   VMK_ReturnStatus ret;

   VMK_ASSERT(vmk_PreemptionIsEnabled() == VMK_FALSE);

   VMK_ASSERT_ON_COMPILE(sizeof(vmk_KeyboardInterruptHandle) ==
                         sizeof(vmklnx_kbd_handle));

   ret = vmk_RegisterInputKeyboardInterruptHandler(
                        (vmk_InputInterruptHandler *)int_info->handler,
                        int_info->irq,
                        int_info->context,
                        int_info->regs,
                        &handle);
   if (ret != VMK_OK) {
      handle = NULL;
   }
   return (vmklnx_kbd_handle)handle;
}
EXPORT_SYMBOL(vmklnx_register_usb_kbd_int_handler);

int
vmklnx_unregister_usb_kbd_int_handler(vmklnx_kbd_handle handle)
{
   VMK_ASSERT(vmk_PreemptionIsEnabled() == VMK_FALSE);

   if (vmk_UnregisterInputKeyboardInterruptHandler((vmk_KeyboardInterruptHandle)handle)
        == VMK_OK) {
      return 0;
   }
   return -1;
}
EXPORT_SYMBOL(vmklnx_unregister_usb_kbd_int_handler);

/*
 * Support for compat_alloc_user_space follows.
 */

/**
 * compat_alloc_user_space - allocate memory from user space
 * @len: len of the memory required
 *
 * Returns memory from the user space of the calling process.
 *
 * ESX Deviation Notes:
 * In ESX, compat_alloc_user_space() actually returns specially marked 
 * kernel memory. You can't directly access the pointer.
 * However, the memory can be passed to copy_to_user(),
 * copy_from_user(), and clear_user() as if it was user memory, so
 * that the implementation is functionally equivalent.
 */

/* _VMKLNX_CODECHECK_: compat_alloc_user_space */
void *compat_alloc_user_space(long len)
{
   struct umem *umem;
   struct task_struct *tp;

   VMK_ASSERT(vmk_PreemptionIsEnabled() == VMK_FALSE);

   /*
    * First check to see if the space has already been allocated.
    */
   tp = vmklnx_GetCurrent();
   umem = tp->umem;
   if (umem && umem->len >= len) {
      return __MarkPtrAsVMKBuffer(&umem->data[0]);
   }

   /*
    * The space is either not there, or its too small.  So allocate some
    * now and set it into our task_struct.
    */
   if (len < MIN_UMEM_SIZE) {
      len = MIN_UMEM_SIZE;
   }
   umem = kmalloc(offsetof(struct umem, data) + len, GFP_KERNEL);
   if (umem == NULL) {
      return NULL;
   }
   umem->len = len;

   if (tp->umem) {
      kfree(tp->umem);
   }

   tp->umem = umem;

   return __MarkPtrAsVMKBuffer(&umem->data[0]);
}
EXPORT_SYMBOL(compat_alloc_user_space);


vmk_Bool
vmklnx_is_physcontig(void *ptr, vmk_uint32 size)
{
   vmk_uintptr_t vaddr, end_vaddr;
   vmk_MA maddr;

   VMK_ASSERT(vmk_PreemptionIsEnabled() == VMK_FALSE);

   vaddr = (vmk_uintptr_t) ptr;
   end_vaddr = vaddr + size;
   vaddr = ALIGN(vaddr, VMK_PAGE_SIZE);
   end_vaddr = roundup(end_vaddr, VMK_PAGE_SIZE);

   // if one page (or no pages), then it is contiguous
   if (vaddr + VMK_PAGE_SIZE >= end_vaddr) {
      return VMK_TRUE;
   }

   maddr = virt_to_phys((void *)vaddr);
   do {
      vaddr += VMK_PAGE_SIZE;
      maddr += VMK_PAGE_SIZE;
      if (maddr != virt_to_phys((void *)vaddr)) {
         return VMK_FALSE;
      }
   } while (vaddr + VMK_PAGE_SIZE < end_vaddr);

   return VMK_TRUE;
}
EXPORT_SYMBOL(vmklnx_is_physcontig);

static VMK_ReturnStatus
vmklnx_mod_init(void)
{
   VMK_ReturnStatus status;

   vmk_ListInit(&module_head);
   status = vmk_SemaCreate(&module_lock, vmk_ModuleCurrentID,
                           "vmklinux-module-list-lock", 1);

   if (status != VMK_OK) {
      VMKLNX_WARN("Semaphore creation failed with error: %s",
                  vmk_StatusToString(status));
   }

   return status;
}

static void
vmklnx_mod_uninit(void)
{
   vmk_SemaDestroy(&module_lock);

   return;
}

VMK_ReturnStatus
vmklnx_register_module(struct module *module, vmk_ModuleID modID)
{
   module_ll *modHdl;

   VMK_ASSERT(vmk_PreemptionIsEnabled() == VMK_FALSE);

   if (module == NULL)
      return VMK_BAD_PARAM;

   if ((module->moduleHandle = modHdl =
                vmk_HeapAlloc(VMK_MODULE_HEAP_ID, sizeof(module_ll))) == NULL) {
      return VMK_NO_MEMORY;
   }

   modHdl->moduleID = modID;
   modHdl->mod = module;
   modHdl->refCnt = 1;
   vmk_SemaLock(&module_lock);
   vmk_ListInsert(&modHdl->moduleList, &module_head);
   vmk_SemaUnlock(&module_lock);

   return VMK_OK;
}
EXPORT_SYMBOL(vmklnx_register_module);

static VMK_ReturnStatus
unregister_module(vmk_ModuleID moduleID)
{
   vmk_ListLinks *llptr = vmk_ListFirst(&module_head);
   module_ll *modlptr;
   VMK_ReturnStatus status = VMK_FAILURE;

   VMK_ASSERT(moduleID != VMK_INVALID_MODULE_ID);

   if (moduleID == VMK_INVALID_MODULE_ID)
      return VMK_BAD_PARAM;

   vmk_SemaLock(&module_lock);
   VMK_LIST_ITER(&module_head, llptr) {
      modlptr = VMK_LIST_ENTRY(llptr, module_ll, moduleList);
      if (modlptr->moduleID == moduleID) {
         if (modlptr->refCnt == 1) {
	    vmk_ListRemove(&modlptr->moduleList);
	    status = VMK_OK;
	 }
         vmk_SemaUnlock(&module_lock);
	 vmk_HeapFree(VMK_MODULE_HEAP_ID, modlptr);
	 return status;
      }
   }
   vmk_SemaUnlock(&module_lock);

   VMK_ASSERT(0); //To catch cases why happened
   return status;
}

VMK_ReturnStatus
vmklnx_unregister_module(struct module *module)
{
   VMK_ASSERT(vmk_PreemptionIsEnabled() == VMK_FALSE);

   if (module == NULL)
      return VMK_BAD_PARAM;

   unregister_module(module->moduleID);

   return VMK_OK;

}
EXPORT_SYMBOL(vmklnx_unregister_module);

static struct module *
module_get_put(vmk_ModuleID moduleID, int flag)
{
   vmk_ListLinks *llptr = vmk_ListFirst(&module_head);
   module_ll *modlptr;

   if (moduleID == VMK_INVALID_MODULE_ID)
      return NULL;

   vmk_SemaLock(&module_lock);
   VMK_LIST_ITER(&module_head, llptr) {
      modlptr = VMK_LIST_ENTRY(llptr, module_ll, moduleList);
      if (modlptr->moduleID == moduleID) {
	 if (flag == MODULE_GET_FLAG) {
            modlptr->refCnt++;
	 } else {
	    /* assume MODULE_PUT_FLAG */
            modlptr->refCnt--;
	 }
         vmk_SemaUnlock(&module_lock);
	 return(modlptr->mod);
      }
   }
   vmk_SemaUnlock(&module_lock);

   return NULL;
}

struct module *
vmklnx_get_module(vmk_ModuleID modID)
{
   VMK_ASSERT(vmk_PreemptionIsEnabled() == VMK_FALSE);
   return module_get_put(modID, MODULE_GET_FLAG);
}
EXPORT_SYMBOL(vmklnx_get_module);

struct module *
vmklnx_put_module(vmk_ModuleID modID)
{
   VMK_ASSERT(vmk_PreemptionIsEnabled() == VMK_FALSE);
   return module_get_put(modID, MODULE_PUT_FLAG);
}
EXPORT_SYMBOL(vmklnx_put_module);


/*
 *----------------------------------------------------------------------
 *
 * vmklnx_mem_pool_get_parent --
 *
 *     Gets the parent memory pool for the current vmklinux instance.
 *
 * Results:
 *	     VMK_OK on success, error code otherwise.
 *
 * Side effects:
 *	     Sets the parent_mem_pool pointer.
 *
 *----------------------------------------------------------------------
 */

VMK_ReturnStatus
vmklnx_mem_pool_get_parent(vmk_MemPool *parent_mem_pool)
{
   VMK_ASSERT(THIS_MODULE->parent_mem_pool != VMK_MEMPOOL_INVALID);
   VMK_ASSERT(vmk_PreemptionIsEnabled() == VMK_FALSE);

   if (THIS_MODULE->parent_mem_pool != VMK_MEMPOOL_INVALID) {
      *parent_mem_pool = THIS_MODULE->parent_mem_pool;
      return VMK_OK;
   }
   *parent_mem_pool = VMK_MEMPOOL_INVALID;
   return VMK_NOT_FOUND;
}
EXPORT_SYMBOL(vmklnx_mem_pool_get_parent);


/*
 *----------------------------------------------------------------------
 *
 * vmklnx_mem_pool_parent_init --
 *
 *     Creates the parent memory pool for the current vmklinux instance
 *     as a child of the vmkplexer memory pool.
 *
 * Results:
 *	     VMK_OK on success, error code otherwise.
 *
 * Side effects:
 *	     Creates vmklinux memory pool.
 *      Sets the vmklinux parent_mem_pool pointer.
 *
 *----------------------------------------------------------------------
 */

VMK_ReturnStatus
vmklnx_mem_pool_parent_init(void)
{
   VMK_ReturnStatus status;
   vmk_MemPool vmkplexer_mem_pool;
   vmk_MemPoolProps mpool_props;

   status = vmkplxr_GetMemPool(&vmkplexer_mem_pool);
   VMK_ASSERT(status == VMK_OK);
   
   mpool_props.module = THIS_MODULE->moduleID;
   mpool_props.parentMemPool = vmkplexer_mem_pool;
   mpool_props.memPoolType = VMK_MEM_POOL_PARENT;
   mpool_props.resourceProps.reservation = 0;
   mpool_props.resourceProps.limit = VMK_MEMPOOL_NO_LIMIT;
   status = vmk_NameFormat(&mpool_props.name, "%s",
                           VMKLNX_MODIFY_NAME(vmklinux));
   VMK_ASSERT(status == VMK_OK);
   return vmk_MemPoolCreate(&mpool_props, &(THIS_MODULE->parent_mem_pool));
}


/*
 * The following symbols are defined in 
 * vmkdrivers/src_92/vmklinux_92/linux/arch/x86_64/lib/thunk.S,
 * since it is an assembly file, the EXPORT_SYMBOL calls cannot 
 * not be done there, hence making the calls here.
 */
EXPORT_SYMBOL(__down_failed);
EXPORT_SYMBOL(__down_failed_interruptible);
EXPORT_SYMBOL(__down_failed_trylock);
EXPORT_SYMBOL(__read_lock_failed);
EXPORT_SYMBOL(__write_lock_failed);
EXPORT_SYMBOL(__up_wakeup);
EXPORT_SYMBOL(__raw_spin_failed);


/**
 *  mutex_lock - Lock the mutex
 *  @m: mutex in question
 *
 *  Lock the mutex.
 *
 */
/* _VMKLNX_CODECHECK_: mutex_lock */
void
mutex_lock(struct mutex *m)
{
   VMK_ASSERT(!in_interrupt());

   down(&m->lock);
   VMKLNX_DEBUG_ONLY( LinuxTask_AddMutexRecord(m) );
}
EXPORT_SYMBOL(mutex_lock);


/**
 *  mutex_unlock - Unlock mutex
 *  @m: mutex in question
 *
 *  Unlock the mutex
 *
 */
/* _VMKLNX_CODECHECK_: mutex_unlock */
void
mutex_unlock(struct mutex *m)
{
   VMKLNX_DEBUG_ONLY( LinuxTask_DeleteMutexRecord(m); )
   up(&m->lock);
}
EXPORT_SYMBOL(mutex_unlock);


/**
 *  mutex_trylock - Attempt to acquire a mutex lock without blocking.
 *  @m: mutex to lock
 *
 *  Attempts to acquire a mutex lock.  If the lock is not immediately
 *  available, this function returns immediately and indicates failure
 *  in its return value.
 *
 *  RETURN VALUE:
 *  1 if lock acquisition succeeds, 0 otherwise.
 */
/* _VMKLNX_CODECHECK_: mutex_trylock */
int
mutex_trylock(struct mutex *m)
{
   VMK_ASSERT(!in_interrupt());

   if (down_trylock(&m->lock) == 0) {
      VMKLNX_DEBUG_ONLY( LinuxTask_AddMutexRecord(m) );
      return 1;
   } else {
      return 0;
   }
}
EXPORT_SYMBOL(mutex_trylock);


/**
 *  mutex_lock_interruptible - Acquire a mutex lock, allowing a signal to interrupt the wait.
 *  @m: mutex whose lock to acquire
 *
 *  Attempts to acquire a mutex's lock from an interruptible context.
 *  If the lock is not immediately available, this function will block
 *  and wait until the lock is acquired, unless a signal is received that
 *  interrupts the acquisition.
 *
 *  ESX DEVIATION NOTES:
 *  Keyboard interrupts from a userworld will not interrupt waiting.
 *
 *  RETURN VALUE:
 *  0 upon success, -EINTR if acquisition was interrupted
 */
/* _VMKLNX_CODECHECK_: mutex_lock_interruptible */
int
mutex_lock_interruptible(struct mutex *m)
{
   int status;

   VMK_ASSERT(!in_interrupt());

   status = down_interruptible(&m->lock);

   VMKLNX_DEBUG_ONLY( if (status == 0) LinuxTask_AddMutexRecord(m) );

   return status;
}
EXPORT_SYMBOL(mutex_lock_interruptible);
